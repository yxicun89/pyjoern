# cleanãŒä½¿ã£ã¦ã‚‹æ–¹

# # ç ”ç©¶ä¼šã®æ™‚ã«ç¤ºã—ã¦ã„ãŸã‚³ãƒ¼ãƒ‰
# # 11æ¬¡å…ƒã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™

# import numpy as np
# from sklearn.cluster import KMeans
# from sklearn.metrics.pairwise import cosine_distances
# from sklearn.datasets import make_blobs, make_circles, make_moons, load_iris, load_wine
# from sklearn.decomposition import PCA
# import matplotlib.pyplot as plt
# import os
# from datetime import datetime

# # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# try:
#     import seaborn as sns
#     import pandas as pd
#     ADVANCED_VIZ_AVAILABLE = True
# except ImportError:
#     ADVANCED_VIZ_AVAILABLE = False
#     print("âš ï¸ seaborn/pandasãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªå¯è¦–åŒ–ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
#     print("   é«˜åº¦ãªå¯è¦–åŒ–ã«ã¯: pip install seaborn pandas ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# # ext_cfg_dfg_feature.pyã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# try:
#     from ext_cfg_dfg_feature import (
#         extract_integrated_features_vector,
#         batch_extract_integrated_features,
#         find_files_in_directory,
#         load_feature_vectors,
#         save_feature_vectors,
#         check_cache_validity,
#         analyze_file_groups
#     )
#     FEATURE_EXTRACTION_AVAILABLE = True
# except ImportError as e:
#     print(f"âŒ ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
#     print("ext_cfg_dfg_feature.pyãŒåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
#     FEATURE_EXTRACTION_AVAILABLE = False

# # --- ç‰¹å¾´é‡ã®é‡ã¿ã‚’å®šç¾© ---
# # connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity
# # variable_count, total_reads, total_writes, max_reads, max_writes ã«å¯¾å¿œ
# FEATURE_WEIGHTS = np.array([
#     1.0, # connected_components
#     1.0, # loop_statements
#     1.0, # conditional_statements
#     1.0, # cycles
#     1.0, # paths
#     1.0, # cyclomatic_complexity
#     0.6, # variable_count
#     0.1, # total_reads
#     0.1, # total_writes
#     0.1, # max_reads
#     0.1  # max_writes
# ])

# # --- è·é›¢é–¢æ•°ï¼ˆé‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã€ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã€ã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰ ---
# def dist(c, s, metric='euclidean', weights=None):
#     if metric == 'euclidean':
#         if weights is None:
#             return np.linalg.norm(c - s)
#         else:
#             # é‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢: sqrt(sum(w_i * (c_i - s_i)^2))
#             return np.sqrt(np.sum(weights * (c - s)**2))
#     elif metric == 'manhattan':
#         if weights is None:
#             return np.sum(np.abs(c - s))
#         else:
#             # é‡ã¿ä»˜ããƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢: sum(w_i * |c_i - s_i|)
#             return np.sum(weights * np.abs(c - s))
#     elif metric == 'cosine':
#         if weights is None:
#             return cosine_distances([c], [s])[0][0]
#         else:
#             # é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³è·é›¢: é‡ã¿ã‚’sqrt(w_i)ã§ç‰¹å¾´é‡ã«é©ç”¨ã—ã¦ã‹ã‚‰ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’è¨ˆç®—
#             c_w = c * np.sqrt(weights)
#             s_w = s * np.sqrt(weights)
#             return cosine_distances([c_w], [s_w])[0][0]
#     else:
#         raise ValueError(f"æœªçŸ¥ã®è·é›¢é–¢æ•°ã§ã™: {metric}")

# # --- K-means++ åˆæœŸåŒ– ---
# def initialize_centroids(X_data, k):
#     kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)
#     kmeans.fit(X_data)
#     return kmeans.cluster_centers_

# # --- ä¸€èˆ¬çš„ãªK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ---
# def general_kmeans_algorithm(X_data, k, metric='euclidean', weights=None, max_iterations=100):
#     C = initialize_centroids(X_data, k)

#     for iteration in range(max_iterations):
#         # ã‚¹ãƒ†ãƒƒãƒ— 1: å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æœ€ã‚‚è¿‘ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦ã‚‹
#         labels = np.zeros(len(X_data), dtype=int)
#         for i, S in enumerate(X_data):
#             dists = [dist(c, S, metric, weights=weights) for c in C]
#             labels[i] = np.argmin(dists)

#         # ã‚¹ãƒ†ãƒƒãƒ— 2: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‰²ã‚Šå½“ã¦ã«åŸºã¥ã„ã¦ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ›´æ–°
#         new_C = np.zeros((k, X_data.shape[1]))
#         for i in range(k):
#             points_in_cluster = X_data[labels == i]
#             if len(points_in_cluster) > 0:
#                 new_C[i] = np.mean(points_in_cluster, axis=0)
#             else:
#                 # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒç©ºã«ãªã£ãŸå ´åˆã€ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ ã«å†åˆæœŸåŒ–ã™ã‚‹
#                 min_val = np.min(X_data, axis=0)
#                 max_val = np.max(X_data, axis=0)
#                 new_C[i] = np.random.uniform(min_val, max_val, X_data.shape[1])

#         # åæŸåˆ¤å®š: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒã»ã¨ã‚“ã©å¤‰åŒ–ã—ãªããªã£ãŸã‚‰åœæ­¢
#         if np.allclose(C, new_C):
#             break

#         C = new_C

#     # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
#     final_labels = np.zeros(len(X_data), dtype=int)
#     for i, S in enumerate(X_data):
#         dists = [dist(c, S, metric, weights=weights) for c in C]
#         final_labels[i] = np.argmin(dists)

#     return C, final_labels

# # --- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæ­£è§£åˆ¤å®šé–¢æ•°åˆ©ç”¨)---
# # def clustering_algorithm_with_correctness(X_data, k, is_correct_fn, metric='euclidean', weights=None):
# #     C = initialize_centroids(X_data, k)
# #     N = np.zeros(k) # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®æ•°

# #     for S in X_data:
# #         # å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ S ã‚’æœ€ã‚‚è¿‘ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦ã‚‹
# #         dists = [dist(c, S, metric, weights=weights) for c in C]
# #         min_c = np.argmin(dists) # å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

# #         N[min_c] += 1

# #         # æ­£è§£åˆ¤å®šé–¢æ•°ãŒTrueã‚’è¿”ã—ãŸå ´åˆã«ã®ã¿ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ›´æ–°
# #         if is_correct_fn(S, min_c):
# #             # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ä¼¼ãŸã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ›´æ–°ï¼ˆ1ç‚¹ã”ã¨ã®ç§»å‹•å¹³å‡ï¼‰
# #             C[min_c] = C[min_c] + (1 / N[min_c]) * (S - C[min_c])

# #     # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
# #     final_labels = np.zeros(len(X_data), dtype=int)
# #     for i, S in enumerate(X_data):
# #         dists = [dist(c, S, metric, weights=weights) for c in C]
# #         final_labels[i] = np.argmin(dists)

# #     return C, final_labels

# # --- æ­£è§£åˆ¤å®šé–¢æ•°ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ï¼ˆæ•™å¸«ã‚ã‚Šï¼‰ ---
# # def is_correct_fn_factory(true_centers):
# #     if true_centers is None:
# #         # çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒãŒãªã„å ´åˆã¯ã€å¸¸ã«Trueã‚’è¿”ã™
# #         print("Warning: No true_centers provided for correctness check. The algorithm will always consider an assignment 'correct'. This might not be the intended use.")
# #         return lambda S, assigned_cluster_idx: True

# #     def is_correct(S, assigned_cluster_idx):
# #         # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ S ãŒã©ã®çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ã‹ã‚’åˆ¤æ–­
# #         true_dists = [np.linalg.norm(tc - S) for tc in true_centers]
# #         correct_cluster_idx = np.argmin(true_dists)

# #         # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå‰²ã‚Šå½“ã¦ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒä¸€è‡´ã™ã‚‹ã‹ã©ã†ã‹ã‚’è¿”ã™
# #         return assigned_cluster_idx == correct_cluster_idx
# #     return is_correct

# # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–¢æ•° ---
# def create_dataset(dataset_name: str, n_samples: int = 300):
#     if dataset_name == 'real_code_features':
#         # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
#         if not FEATURE_EXTRACTION_AVAILABLE:
#             raise ValueError("ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ext_cfg_dfg_feature.pyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

#         # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã¾ãŸã¯çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
#         target_directory = "../atcoder/submissions_typical90_d_100"

#         if not os.path.exists(target_directory):
#             raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_directory}")

#         # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
#         cache_file = f"feature_cache_{os.path.basename(target_directory)}.json"

#         # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
#         code_files = find_files_in_directory(target_directory)

#         if len(code_files) == 0:
#             raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_directory}")

#         print(f"ğŸ” ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(code_files)}")
#         for i, file in enumerate(code_files[:5]):  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
#             print(f"  {i+1}. {os.path.relpath(file, target_directory)}")
#         if len(code_files) > 5:
#             print(f"  ... ãŠã‚ˆã³ {len(code_files) - 5} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

#         # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯
#         batch_results = None
#         use_cache = False

#         if os.path.exists(cache_file):
#             if check_cache_validity(target_directory, cache_file):
#                 print(f"ğŸ“¦ æœ‰åŠ¹ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {cache_file}")
#                 print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
#                 use_cache = True
#             else:
#                 print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¤ã„ãŸã‚ã€å†æŠ½å‡ºãŒå¿…è¦ã§ã™")

#         if use_cache:
#             # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
#             print(f"ğŸ“‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ä¸­...")
#             cached_data = load_feature_vectors(cache_file)
#             if cached_data:
#                 batch_results = cached_data['data']
#                 print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ {len(batch_results)} ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

#         if batch_results is None:
#             # æ–°è¦æŠ½å‡º
#             print("ğŸ“Š ç‰¹å¾´é‡æŠ½å‡ºä¸­...")
#             batch_results = batch_extract_integrated_features(code_files)

#             # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
#             print(f"ğŸ’¾ ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ä¸­...")
#             save_feature_vectors(batch_results, cache_file, format='json')

#         # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
#         successful_results = [r for r in batch_results if 'error' not in r]

#         if len(successful_results) == 0:
#             raise ValueError("ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

#         print(f"âœ… ç‰¹å¾´é‡æŠ½å‡ºæˆåŠŸ: {len(successful_results)} / {len(code_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

#         # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
#         X = np.array([r['integrated_vector'] for r in successful_results])

#         # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’è‡ªå‹•æ±ºå®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«åŸºã¥ãï¼‰
#         # k_clusters = min(max(2, len(successful_results) // 5), 5)  # 2-5ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
#         k_clusters = 5
#         n_features = 11

#         # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚ã€ä»®ã®ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
#         y_true = np.zeros(len(successful_results))  # ã™ã¹ã¦åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã—ã¦æ‰±ã†

#         # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿å­˜ï¼ˆå¾Œã§å‚ç…§ç”¨ï¼‰
#         file_names = [os.path.basename(r['source_file']) for r in successful_results]

#         # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†æç”¨ï¼‰
#         file_paths = [r['source_file'] for r in successful_results]

#         print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(X)} ã‚µãƒ³ãƒ—ãƒ«, {n_features} ç‰¹å¾´é‡, {k_clusters} ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")

#         # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹æƒ…å ±ã‚’è¿”ã‚Šå€¤ã«å«ã‚ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
#         return X, y_true, k_clusters, n_features, None, file_names, file_paths

#     else:
#         raise ValueError(f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã§ã™: {dataset_name}")

#     # # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’è¨ˆç®—ï¼ˆå®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒãªã„ï¼‰
#     # true_centers_calc = None
#     # if dataset_name == 'random':
#     #     # randomãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒãªã„ãŸã‚ã€true_centersã¯None
#     #     pass
#     # elif dataset_name == 'real_code_features':
#     #     # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒãªã„ãŸã‚ã€true_centersã¯None
#     #     pass
#     # else: # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯y_trueã‚’åŸºã«è¨ˆç®—ã•ã‚Œã‚‹
#     #     true_centers_calc = np.array([X[y_true == i].mean(axis=0) for i in range(k_clusters)])

#     return X, y_true, k_clusters, n_features, true_centers_calc

# # --- æœ€çµ‚çš„ãªã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é–“ã®å¹³å‡æœ€å°è·é›¢ã‚’è¨ˆç®—ã™ã‚‹---
# def calculate_average_min_centroid_distance(final_centroids, true_centers):
#     if final_centroids is None or true_centers is None:
#         return np.nan

#     num_final = final_centroids.shape[0]
#     num_true = true_centers.shape[0]

#     # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒç•°ãªã‚‹å ´åˆã¯è­¦å‘Šï¼ˆãŸã ã—è¨ˆç®—ã¯ç¶šè¡Œï¼‰
#     if num_final != num_true:
#         print(f"Warning: Number of final centroids ({num_final}) does not match number of true centers ({num_true}). "
#               "Distance calculation might be less meaningful.")

#     min_distances = []
#     for f_center in final_centroids:
#         # å„æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«ã¤ã„ã¦ã€å…¨ã¦ã®çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨ã®è·é›¢ã‚’è¨ˆç®—
#         distances_to_true = [np.linalg.norm(f_center - t_center) for t_center in true_centers]
#         min_distances.append(np.min(distances_to_true))

#     return np.mean(min_distances)

# def display_clustering_results(final_labels, C_final, file_names=None, dataset_name="unknown"):
#     """
#     ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’è©³ç´°è¡¨ç¤º

#     Args:
#         final_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ©ãƒ™ãƒ«
#         C_final: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
#         file_names: ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
#         dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
#     """
#     print(f"\nğŸ“Š === {dataset_name.upper()} ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœè©³ç´° ===")

#     unique_labels = np.unique(final_labels)
#     print(f"ğŸ”¢ ç·ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {len(unique_labels)}")
#     print(f"ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(final_labels)}")

#     print(f"\nğŸ¯ å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è©³ç´°:")
#     print("-" * 80)

#     for cluster_id in unique_labels:
#         cluster_indices = np.where(final_labels == cluster_id)[0]
#         cluster_size = len(cluster_indices)

#         print(f"\nğŸ·ï¸  ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster_id}:")
#         print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {cluster_size} ã‚µãƒ³ãƒ—ãƒ« ({cluster_size/len(final_labels)*100:.1f}%)")
#         print(f"   ğŸ¯ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰: {np.round(C_final[cluster_id], 3)}")

#         # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º
#         if file_names:
#             print(f"   ğŸ“„ å«ã¾ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:")
#             cluster_files = [file_names[idx] for idx in cluster_indices]

#             # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
#             cluster_files.sort()
#             for i, filename in enumerate(cluster_files, 1):
#                 print(f"      {i:2d}. {filename}")
#                 if i >= 10 and len(cluster_files) > 10:  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤º
#                     remaining = len(cluster_files) - 10
#                     print(f"      ... ãŠã‚ˆã³ {remaining} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
#                     break
#         else:
#             print(f"   ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {cluster_indices[:10].tolist()}" +
#                   (f" ... (+{len(cluster_indices)-10})" if len(cluster_indices) > 10 else ""))

#     print("-" * 80)

# def preprocess_data_for_visualization(X, file_names=None):
#     """
#     å¯è¦–åŒ–ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆå¤–ã‚Œå€¤å¯¾ç­–ï¼‰
#     æ³¨æ„: ãƒ‘ã‚¹æ•°ãªã©ã®æ¥µç«¯ãªå€¤ã‚‚æ­£å½“ãªã‚³ãƒ¼ãƒ‰ç‰¹å¾´ã®ãŸã‚ã€é€šå¸¸ã¯ä½¿ç”¨ã—ãªã„

#     Args:
#         X: ç‰¹å¾´é‡è¡Œåˆ—
#         file_names: ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ

#     Returns:
#         X_processed: å‰å‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡è¡Œåˆ—
#         outlier_info: å¤–ã‚Œå€¤æƒ…å ±
#     """
#     # æ©Ÿèƒ½ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ - æ­£å½“ãªã‚³ãƒ¼ãƒ‰ç‰¹å¾´ã‚’ç¶­æŒã™ã‚‹ãŸã‚
#     X_processed = X.copy()  # å¤‰æ›´ã›ãšãã®ã¾ã¾è¿”ã™
#     outlier_info = {
#         'outliers_found': False,
#         'n_outliers': 0,
#         'outlier_features': [],
#         'processing_method': 'ãªã—'
#     }

#     # ä»¥ä¸‹ã€å‰å‡¦ç†æ©Ÿèƒ½ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
#     """
#     # ç‰¹å¾´é‡åï¼ˆãƒ‘ã‚¹æ•°ã¯4ç•ªç›®ï¼‰
#     feature_names = [
#         'connected_components', 'loop_statements', 'conditional_statements',
#         'cycles', 'paths', 'cyclomatic_complexity',
#         'variable_count', 'total_reads', 'total_writes',
#         'max_reads', 'max_writes'
#     ]

#     # å¤–ã‚Œå€¤æ¤œå‡ºã¨å‡¦ç†
#     for i, feature_name in enumerate(feature_names):
#         values = X[:, i]

#         # IQRã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
#         Q1 = np.percentile(values, 25)
#         Q3 = np.percentile(values, 75)
#         IQR = Q3 - Q1
#         lower_bound = Q1 - 1.5 * IQR
#         upper_bound = Q3 + 1.5 * IQR

#         # å¤–ã‚Œå€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®š
#         outlier_mask = (values < lower_bound) | (values > upper_bound)
#         n_outliers = np.sum(outlier_mask)

#         if n_outliers > 0:
#             outlier_info['outliers_found'] = True
#             outlier_info['n_outliers'] += n_outliers
#             outlier_info['outlier_features'].append(feature_name)

#             # ç‰¹ã«å•é¡Œã¨ãªã‚‹ãƒ‘ã‚¹æ•°ãªã©ã®å¤§ããªå€¤ã‚’å¯¾å‡¦
#             if feature_name in ['paths', 'cycles'] and np.max(values) > 1000:
#                 # å¯¾æ•°å¤‰æ› + 1ï¼ˆ0å€¤å¯¾ç­–ï¼‰
#                 X_processed[:, i] = np.log1p(values)
#                 outlier_info['processing_method'] = 'å¯¾æ•°å¤‰æ›'

#                 if file_names:
#                     max_idx = np.argmax(values)
#                     print(f"   ğŸ“Š {feature_name}: æœ€å¤§å€¤ {values[max_idx]:.0f} ({file_names[max_idx]}) -> å¯¾æ•°å¤‰æ›é©ç”¨")

#             elif np.max(values) > upper_bound * 2:
#                 # æ¥µç«¯ã«å¤§ãã„å€¤ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
#                 X_processed[:, i] = np.clip(values, lower_bound, upper_bound)
#                 outlier_info['processing_method'] = 'ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°'

#                 if file_names:
#                     extreme_mask = values > upper_bound * 2
#                     extreme_files = [file_names[idx] for idx in np.where(extreme_mask)[0]]
#                     print(f"   âœ‚ï¸ {feature_name}: {len(extreme_files)}å€‹ã®æ¥µç«¯ãªå€¤ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°")
#                     for file in extreme_files[:3]:  # æœ€åˆã®3ã¤ã‚’è¡¨ç¤º
#                         print(f"      - {file}")
#                     if len(extreme_files) > 3:
#                         print(f"      ... ãŠã‚ˆã³{len(extreme_files)-3}å€‹")
#     """

#     return X_processed, outlier_info

# def main(algorithm_type: str, dataset_name: str):
#     # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
#     result = create_dataset(dataset_name)

#     # è¿”ã‚Šå€¤ã®æ•°ã«å¿œã˜ã¦é©åˆ‡ã«åˆ†å‰²
#     if len(result) == 7:
#         X, y_true, k_clusters, n_features, true_centers, file_names, file_paths = result
#     elif len(result) == 6:
#         X, y_true, k_clusters, n_features, true_centers, file_names = result
#         file_paths = None
#     else:
#         X, y_true, k_clusters, n_features, true_centers = result
#         file_names = None
#         file_paths = None

#     # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆå¤–ã‚Œå€¤å¯¾ç­–ï¼‰- ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆæ­£å½“ãªã‚³ãƒ¼ãƒ‰ç‰¹å¾´ã‚’ä¿æŒï¼‰
#     # X_processed, outlier_info = preprocess_data_for_visualization(X, file_names)

#     # å¤–ã‚Œå€¤æƒ…å ±ã‚’è¡¨ç¤º - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
#     # if outlier_info['outliers_found']:
#     #     print(f"\nâš ï¸ å¤–ã‚Œå€¤æ¤œå‡º: {outlier_info['n_outliers']}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§æ¥µç«¯ãªå€¤ã‚’æ¤œå‡º")
#     #     print(f"   å¯¾è±¡ç‰¹å¾´é‡: {outlier_info['outlier_features']}")
#     #     print(f"   å‰å‡¦ç†æ–¹æ³•: {outlier_info['processing_method']}")

#     # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠã¨å®Ÿè¡Œ
#     C_final, final_labels = None, None
#     if algorithm_type == 'general':
#         C_final, final_labels = general_kmeans_algorithm(
#             X_data=X,  # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆå‰å‡¦ç†ãªã—ï¼‰
#             k=k_clusters,
#             metric='euclidean',
#             weights=FEATURE_WEIGHTS if dataset_name == 'real_code_features' else None
#         )
#         algo_title = "General K-means"
#     # elif algorithm_type == 'correctness_guided':
#     #     if true_centers is None and dataset_name != 'random':
#     #         raise ValueError(f"'{dataset_name}' dataset does not have 'true_centers' to run 'correctness_guided' algorithm. "
#     #                          "Please ensure true_centers are generated for this dataset or choose 'general' algorithm.")
#     #     C_final, final_labels = clustering_algorithm_with_correctness(
#     #         X_data=X,
#     #         k=k_clusters,
#     #         is_correct_fn=is_correct_fn_factory(true_centers),
#     #         metric='euclidean',
#     #         weights=FEATURE_WEIGHTS if dataset_name == 'blobs' or dataset_name == 'code_features' else None
#     #     )
#     #     algo_title = "Correctness-Guided K-means"
#     else:
#         raise ValueError(f"ä¸æ˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—ã§ã™: {algorithm_type}")

#     # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢ã®è¨ˆç®—
#     centroid_distance = calculate_average_min_centroid_distance(C_final, true_centers)

#     # çµæœã®å‡ºåŠ›
#     print(f"--- {dataset_name.capitalize()} Dataset Results ({algo_title}, k={k_clusters}) ---")
#     print(f"æœ€çµ‚çš„ãªã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰:\n", np.round(C_final, 2))
#     if true_centers is not None and not np.isnan(centroid_distance):
#         print(f"æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é–“ã®å¹³å‡æœ€å°è·é›¢: {centroid_distance:.4f}")
#     elif dataset_name == 'random':
#         print("ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒãªã„ãŸã‚ã€ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢ã¯è¨ˆç®—ã•ã‚Œã¾ã›ã‚“ã€‚")
#     else:
#         print("çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢ã¯è¨ˆç®—ã•ã‚Œã¾ã›ã‚“ã€‚")
#     print("-" * 50)

#     # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©³ç´°è¡¨ç¤º
#     display_clustering_results(final_labels, C_final, file_names, dataset_name)

#     # å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯PCAã§æ¬¡å…ƒå‰Šæ¸›ï¼‰
#     visualize_clustering_results(X, y_true, final_labels, C_final, true_centers,
#                                dataset_name, algo_title, k_clusters, n_features, file_paths)

# def visualize_clustering_results(X, y_true, final_labels, C_final, true_centers,
#                                dataset_name, algo_title, k_clusters, n_features, file_paths=None):
#     """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘å¯¾å¿œï¼‰"""

#     # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª1éšå±¤ï¼‰
#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#     output_dir = f"clustering_results_{timestamp}"
#     os.makedirs(output_dir, exist_ok=True)

#     # 2æ¬¡å…ƒä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯PCAã§æ¬¡å…ƒå‰Šæ¸›
#     if n_features > 2:
#         pca = PCA(n_components=2)
#         X_2d = pca.fit_transform(X)
#         C_final_2d = pca.transform(C_final)
#         if true_centers is not None:
#             true_centers_2d = pca.transform(true_centers)
#         else:
#             true_centers_2d = None

#         # PCAæƒ…å ±ã‚’è¡¨ç¤º
#         explained_var_ratio = pca.explained_variance_ratio_
#         total_explained_var = np.sum(explained_var_ratio)
#         print(f"\nğŸ“Š PCAæ¬¡å…ƒå‰Šæ¸›æƒ…å ±:")
#         print(f"   PC1ã®èª¬æ˜åˆ†æ•£æ¯”: {explained_var_ratio[0]:.3f} ({explained_var_ratio[0]*100:.1f}%)")
#         print(f"   PC2ã®èª¬æ˜åˆ†æ•£æ¯”: {explained_var_ratio[1]:.3f} ({explained_var_ratio[1]*100:.1f}%)")
#         print(f"   åˆè¨ˆèª¬æ˜åˆ†æ•£æ¯”: {total_explained_var:.3f} ({total_explained_var*100:.1f}%)")

#         plot_title_suffix = f" (PCA 2D: {total_explained_var*100:.1f}% variance)"
#     else:
#         X_2d = X
#         C_final_2d = C_final
#         true_centers_2d = true_centers
#         plot_title_suffix = ""

#     # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®è‰²åˆ†ã‘æƒ…å ±ã‚’å–å¾—
#     pattern_groups = None
#     pattern_colors = None
#     pattern_labels = None

#     if file_paths is not None and dataset_name == 'real_code_features':
#         # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
#         target_directory = "../atcoder/submissions_typical90_d_100"

#         # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
#         pattern_groups = analyze_file_groups(file_paths, target_directory)

#         # è‰²ã®è¨­å®š
#         color_palette = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'olive', 'cyan']
#         pattern_colors = {}
#         color_idx = 0

#         for group_name in pattern_groups.keys():
#             if group_name == 'other':
#                 pattern_colors[group_name] = 'gray'
#             else:
#                 pattern_colors[group_name] = color_palette[color_idx % len(color_palette)]
#                 color_idx += 1

#         # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
#         file_to_group = {}
#         for group_name, group_files in pattern_groups.items():
#             for file_info in group_files:
#                 file_to_group[file_info['file_path']] = group_name

#         pattern_labels = [file_to_group.get(fp, 'other') for fp in file_paths]

#     plt.figure(figsize=(12, 5))

#     # å¯†é›†åº¦ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šã‚’èª¿æ•´
#     n_points = len(X_2d)
#     if n_points > 100:
#         point_size = max(20, 100 - n_points // 10)  # ç‚¹æ•°ãŒå¤šã„ã»ã©å°ã•ã
#         alpha_val = max(0.4, 1.0 - n_points / 500)  # ç‚¹æ•°ãŒå¤šã„ã»ã©é€æ˜ã«
#     else:
#         point_size = 50
#         alpha_val = 0.7

#     # å·¦å´: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘ï¼ˆã¾ãŸã¯çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰
#     plt.subplot(1, 2, 1)
#     if pattern_groups is not None:
#         # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
#         for group_name in pattern_groups.keys():
#             group_indices = [i for i, label in enumerate(pattern_labels) if label == group_name]
#             if group_indices:
#                 group_points = X_2d[group_indices]
#                 plt.scatter(group_points[:, 0], group_points[:, 1],
#                            c=pattern_colors[group_name],
#                            label=f'{group_name} ({len(group_indices)})',
#                            alpha=alpha_val, s=point_size)

#         plt.title(f"Pattern-based Grouping\n{dataset_name.capitalize()} Dataset{plot_title_suffix}")
#         plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)

#         # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®è©³ç´°æƒ…å ±ã‚’ãƒ—ãƒ­ãƒƒãƒˆä¸Šã«è¡¨ç¤º
#         if pattern_groups:
#             info_text = "Pattern Colors:\n"
#             for group_name in sorted(pattern_groups.keys()):
#                 group_count = len([f for f in pattern_groups[group_name] if f['file_path'] in file_paths])
#                 info_text += f"â€¢ {group_name}: {group_count} files\n"

#             # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã‚’å·¦ä¸‹ã«é…ç½®
#             plt.text(0.02, 0.02, info_text, transform=plt.gca().transAxes,
#                     fontsize=8, verticalalignment='bottom',
#                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

#     elif y_true is not None and dataset_name != 'random':
#         scatter1 = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_true, cmap='viridis', alpha=alpha_val, s=point_size)
#         plt.title(f"True Clusters\n{dataset_name.capitalize()} Dataset{plot_title_suffix}")
#         plt.colorbar(scatter1, label='True Cluster')

#         # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
#         if true_centers_2d is not None:
#             plt.scatter(true_centers_2d[:, 0], true_centers_2d[:, 1],
#                        c='blue', s=200, marker='o', edgecolor='black',
#                        label='True Centers', alpha=0.8)
#             plt.legend()
#     else:
#         plt.scatter(X_2d[:, 0], X_2d[:, 1], c='gray', alpha=alpha_val, s=point_size)
#         plt.title(f"Original Data\n{dataset_name.capitalize()} Dataset{plot_title_suffix}")

#     plt.xlabel("Component 1" if n_features > 2 else "Feature 1")
#     plt.ylabel("Component 2" if n_features > 2 else "Feature 2")
#     plt.grid(True, alpha=0.3)

#     # å³å´: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
#     plt.subplot(1, 2, 2)
#     scatter2 = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=final_labels, cmap='tab10', alpha=alpha_val, s=point_size)
#     plt.title(f"{algo_title} Results\n{dataset_name.capitalize()} Dataset{plot_title_suffix}")
#     plt.colorbar(scatter2, label='Predicted Cluster')

#     # æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
#     plt.scatter(C_final_2d[:, 0], C_final_2d[:, 1],
#                c='red', s=200, marker='X', edgecolor='black',
#                label='Final Centroids', alpha=0.9)

#     # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¥ã®è©³ç´°æƒ…å ±ã‚’ãƒ—ãƒ­ãƒƒãƒˆä¸Šã«è¡¨ç¤º
#     unique_clusters = np.unique(final_labels)
#     cluster_info_text = "Cluster Info:\n"
#     for cluster_id in unique_clusters:
#         cluster_count = np.sum(final_labels == cluster_id)
#         cluster_info_text += f"â€¢ Cluster {cluster_id}: {cluster_count} files\n"

#     # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã‚’å³ä¸‹ã«é…ç½®
#     plt.text(0.98, 0.02, cluster_info_text, transform=plt.gca().transAxes,
#             fontsize=8, verticalalignment='bottom', horizontalalignment='right',
#             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

#     # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚‚è¡¨ç¤ºï¼ˆæ¯”è¼ƒç”¨ï¼‰
#     if true_centers_2d is not None:
#         plt.scatter(true_centers_2d[:, 0], true_centers_2d[:, 1],
#                    c='blue', s=150, marker='o', edgecolor='black',
#                    label='True Centers', alpha=0.7)

#     plt.xlabel("Component 1" if n_features > 2 else "Feature 1")
#     plt.ylabel("Component 2" if n_features > 2 else "Feature 2")
#     plt.legend()
#     plt.grid(True, alpha=0.3)

#     plt.tight_layout()

#     # ç”»åƒã¨ã—ã¦ä¿å­˜ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
#     filename = os.path.join(output_dir, f"clustering_result_{dataset_name}_{algo_title.lower().replace(' ', '_').replace('-', '_')}_{timestamp}.png")
#     plt.savefig(filename, dpi=150, bbox_inches='tight')
#     print(f"ğŸ“¸ å¯è¦–åŒ–çµæœã‚’ '{filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

#     plt.show()
#     # ãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®ç¯„å›²ã‚’ç¢ºèª
#     x_data_min = np.min(X_2d[:, 0])
#     x_data_max = np.max(X_2d[:, 0])
#     y_data_min = np.min(X_2d[:, 1])
#     y_data_max = np.max(X_2d[:, 1])

#     print(f"\nğŸ“ˆ å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
#     print(f"  PC1ç¯„å›²: [{x_data_min:.1f}, {x_data_max:.1f}]")
#     print(f"  PC2ç¯„å›²: [{y_data_min:.1f}, {y_data_max:.1f}]")

#     # ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§å€¤ã«åŸºã¥ã„ã¦ä¸Šé™ã‚’è¨­å®šï¼ˆå°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰
#     x_max_fixed = max(500, x_data_max * 1.1)  # æœ€ä½500ã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æœ€å¤§å€¤ã®1.1å€
#     y_max_fixed = max(50, y_data_max * 1.1)   # æœ€ä½50ã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æœ€å¤§å€¤ã®1.1å€
#     x_min = x_data_min  # æ¨ªè»¸ä¸‹é™ã¯ãƒ‡ãƒ¼ã‚¿ã®æœ€å°å€¤
#     y_min = y_data_min  # ç¸¦è»¸ä¸‹é™ã¯ãƒ‡ãƒ¼ã‚¿ã®æœ€å°å€¤

#     x_max = x_max_fixed
#     y_max = y_max_fixed    # å¯†é›†ç¯„å›²å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆä¸¡è»¸ã¨ã‚‚ä¸Šé™ã®ã¿ã€ä¸‹é™ãªã—ï¼‰
#     in_range_mask = (X_2d[:, 0] <= x_max) & (X_2d[:, 1] <= y_max)
#     out_range_count = np.sum(~in_range_mask)
#     in_range_count = np.sum(in_range_mask)

#     print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿é©å¿œå‹ä¸Šé™è¨­å®šã®ç¯„å›²åˆ†æ:")
#     print(f"  ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ç¯„å›²: PC1=[{x_min:.1f}, {x_max:.1f}] (ä¸Šé™: {x_max_fixed:.0f})")
#     print(f"  ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ç¯„å›²: PC2=[{y_min:.1f}, {y_max:.1f}] (ä¸Šé™: {y_max_fixed:.0f})")
#     print(f"  å¯†é›†ç¯„å›²å†…: {in_range_count} ãƒ•ã‚¡ã‚¤ãƒ«")
#     print(f"  å¯†é›†ç¯„å›²å¤–: {out_range_count} ãƒ•ã‚¡ã‚¤ãƒ«")
#     print(f"ğŸ“ å¯†é›†ç¯„å›²ã®è©³ç´°è¡¨ç¤ºã‚’ä½œæˆä¸­...")

#     if in_range_count > 5:  # å¯†é›†ç¯„å›²ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
#         # å¯†é›†ç¯„å›²ç”¨ã®ç‚¹ã‚µã‚¤ã‚ºã¨é€æ˜åº¦ã‚’èª¿æ•´
#         dense_point_size = min(80, max(30, 300 // in_range_count))  # å¯†é›†åº¦ã«å¿œã˜ã¦èª¿æ•´
#         dense_alpha = max(0.6, min(0.9, 50 / in_range_count))  # é€æ˜åº¦èª¿æ•´

#         plt.figure(figsize=(15, 6))  # ã‚ˆã‚Šå¤§ããªãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã‚µã‚¤ã‚º

#         # å·¦å´: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘ï¼ˆå¯†é›†ç¯„å›²ã®ã¿ï¼‰
#         plt.subplot(1, 2, 1)
#         if pattern_groups is not None:
#             # å¯†é›†ç¯„å›²å†…ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ—ãƒ­ãƒƒãƒˆ
#             for group_name in pattern_groups.keys():
#                 group_indices = [i for i, label in enumerate(pattern_labels)
#                                if label == group_name and in_range_mask[i]]
#                 if group_indices:
#                     group_points = X_2d[group_indices]
#                     plt.scatter(group_points[:, 0], group_points[:, 1],
#                                c=pattern_colors[group_name],
#                                label=f'{group_name} ({len(group_indices)})',
#                                alpha=dense_alpha, s=dense_point_size, edgecolors='black', linewidth=0.3)

#             plt.title(f"Pattern-based Grouping (Dense Region Focus)\n{in_range_count} files in range", fontsize=12)
#             plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)

#             # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®è©³ç´°æƒ…å ±ã‚’ãƒ—ãƒ­ãƒƒãƒˆä¸Šã«è¡¨ç¤ºï¼ˆå¯†é›†ç¯„å›²å†…ã®ã¿ï¼‰
#             info_text = "Dense Region Patterns:\n"
#             for group_name in sorted(pattern_groups.keys()):
#                 group_in_range = len([i for i, label in enumerate(pattern_labels)
#                                     if label == group_name and in_range_mask[i]])
#                 if group_in_range > 0:
#                     info_text += f"â€¢ {group_name}: {group_in_range} files\n"

#             plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes,
#                     fontsize=9, verticalalignment='top',
#                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'))

#         # è¡¨ç¤ºç¯„å›²ã‚’å¯†é›†ç¯„å›²ã«è¨­å®š
#         plt.xlim(x_min, x_max)
#         plt.ylim(y_min, y_max)
#         plt.xlabel("Component 1" if n_features > 2 else "Feature 1", fontsize=11)
#         plt.ylabel("Component 2" if n_features > 2 else "Feature 2", fontsize=11)
#         plt.grid(True, alpha=0.4)

#         # å³å´: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆå¯†é›†ç¯„å›²ã®ã¿ï¼‰
#         plt.subplot(1, 2, 2)
#         # å¯†é›†ç¯„å›²å†…ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ—ãƒ­ãƒƒãƒˆ
#         in_range_X_2d = X_2d[in_range_mask]
#         in_range_labels = final_labels[in_range_mask]

#         scatter3 = plt.scatter(in_range_X_2d[:, 0], in_range_X_2d[:, 1],
#                               c=in_range_labels, cmap='tab10',
#                               alpha=dense_alpha, s=dense_point_size, edgecolors='black', linewidth=0.3)
#         plt.title(f"{algo_title} Results (Dense Region Focus)\n{in_range_count} files in range", fontsize=12)
#         plt.colorbar(scatter3, label='Predicted Cluster')

#         # å¯†é›†ç¯„å›²å†…ã«ã‚ã‚‹ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
#         in_range_centroids_mask = (C_final_2d[:, 0] >= x_min) & (C_final_2d[:, 0] <= x_max) & \
#                                   (C_final_2d[:, 1] >= y_min) & (C_final_2d[:, 1] <= y_max)
#         if np.any(in_range_centroids_mask):
#             plt.scatter(C_final_2d[in_range_centroids_mask, 0], C_final_2d[in_range_centroids_mask, 1],
#                        c='red', s=250, marker='X', edgecolor='black', linewidth=2,
#                        label='Centroids (in dense region)', alpha=1.0)

#         # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¥ã®è©³ç´°æƒ…å ±ï¼ˆå¯†é›†ç¯„å›²å†…ã®ã¿ï¼‰
#         unique_clusters_in_range = np.unique(in_range_labels)
#         cluster_info_text = "Dense Region Clusters:\n"
#         for cluster_id in unique_clusters_in_range:
#             cluster_count = np.sum(in_range_labels == cluster_id)
#             cluster_info_text += f"â€¢ Cluster {cluster_id}: {cluster_count} files\n"

#         plt.text(0.02, 0.98, cluster_info_text, transform=plt.gca().transAxes,
#                 fontsize=9, verticalalignment='top',
#                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'))

#         # è¡¨ç¤ºç¯„å›²ã‚’å¯†é›†ç¯„å›²ã«è¨­å®š
#         plt.xlim(x_min, x_max)
#         plt.ylim(y_min, y_max)
#         plt.xlabel("Component 1" if n_features > 2 else "Feature 1", fontsize=11)
#         plt.ylabel("Component 2" if n_features > 2 else "Feature 2", fontsize=11)
#         plt.legend(fontsize=9)
#         plt.grid(True, alpha=0.4)

#         plt.tight_layout()

#         # å¯†é›†ç¯„å›²ã®ç”»åƒã¨ã—ã¦ä¿å­˜
#         dense_filename = os.path.join(output_dir, f"clustering_result_dense_{dataset_name}_{algo_title.lower().replace(' ', '_').replace('-', '_')}_{timestamp}.png")
#         plt.savefig(dense_filename, dpi=200, bbox_inches='tight')
#         print(f"ğŸ“¸ å¯†é›†ç¯„å›²ã®å¯è¦–åŒ–çµæœã‚’ '{dense_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")
#         print(f"   å¯†é›†ç¯„å›²è¨­å®š: ç‚¹ã‚µã‚¤ã‚º={dense_point_size}, é€æ˜åº¦={dense_alpha:.2f}")

#         plt.show()
#     else:
#         print(f"âš ï¸ å¯†é›†ç¯„å›²å†…ã®ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™({in_range_count}å€‹)ã€‚å¯†é›†ç¯„å›²ã‚°ãƒ©ãƒ•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœã®å‡ºåŠ›
#     if pattern_groups is not None:
#         print(f"\nğŸ¨ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ:")
#         for group_name, group_files in pattern_groups.items():
#             group_count = len([f for f in group_files if f['file_path'] in file_paths])
#             print(f"  {group_name}: {group_count} ãƒ•ã‚¡ã‚¤ãƒ« (è‰²: {pattern_colors[group_name]})")

#     # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
#     print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒçµ±è¨ˆ:")
#     print(f"  ãƒ‡ãƒ¼ã‚¿ç¯„å›² PC1: [{np.min(X_2d[:, 0]):.2f}, {np.max(X_2d[:, 0]):.2f}]")
#     print(f"  ãƒ‡ãƒ¼ã‚¿ç¯„å›² PC2: [{np.min(X_2d[:, 1]):.2f}, {np.max(X_2d[:, 1]):.2f}]")
#     print(f"  æ¨™æº–åå·® PC1: {np.std(X_2d[:, 0]):.2f}")
#     print(f"  æ¨™æº–åå·® PC2: {np.std(X_2d[:, 1]):.2f}")

# if __name__ == '__main__':

#     # --- å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ---
#     if FEATURE_EXTRACTION_AVAILABLE:
#         print("\n=== Real Code Features Dataset: å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ç‰¹å¾´é‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ===")
#         try:
#             # ä¸€èˆ¬çš„ãªK-meansã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
#             main(algorithm_type='general', dataset_name='real_code_features')
#             # main(algorithm_type='correctness_guided', dataset_name='real_code_features')  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
#         except Exception as e:
#             print(f"âŒ å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")
#             print("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
#     else:
#         print("âš ï¸ ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
