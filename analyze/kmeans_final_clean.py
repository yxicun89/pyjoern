# ã“ã‚ŒãŒç¾æ™‚ç‚¹ã§ã®å®Œæˆç‰ˆ
# 11æ¬¡å…ƒã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_distances
from sklearn.datasets import make_blobs, make_circles, make_moons, load_iris, load_wine
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import os
from datetime import datetime

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import seaborn as sns
    import pandas as pd
    ADVANCED_VIZ_AVAILABLE = True
except ImportError:
    ADVANCED_VIZ_AVAILABLE = False

# JSONãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json

# æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sklearn.manifold import TSNE
    TSNE_AVAILABLE = True
except ImportError:
    TSNE_AVAILABLE = False

try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False

# ext_cfg_dfg_feature.pyã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from ext_cfg_dfg_feature import (
        extract_integrated_features_vector,
        batch_extract_integrated_features,
        find_files_in_directory,
        load_feature_vectors,
        save_feature_vectors,
        check_cache_validity,
        analyze_file_groups
    )
    FEATURE_EXTRACTION_AVAILABLE = True
except ImportError as e:
    FEATURE_EXTRACTION_AVAILABLE = False

# --- ç‰¹å¾´é‡ã®é‡ã¿ã‚’å®šç¾© ---
# connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity
# variable_count, total_reads, total_writes, max_reads, max_writes ã«å¯¾å¿œ
FEATURE_WEIGHTS = np.array([
    1.0, # connected_components
    1.0, # loop_statements
    1.0, # conditional_statements
    1.0, # cycles
    1.0, # paths
    1.0, # cyclomatic_complexity
    0.6, # variable_count
    0.1, # total_reads
    0.1, # total_writes
    0.1, # max_reads
    0.1  # max_writes
])

# --- è·é›¢é–¢æ•°ï¼ˆé‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã€ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã€ã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰ ---
def dist(c, s, metric='euclidean', weights=None):
    if metric == 'euclidean':
        if weights is None:
            return np.linalg.norm(c - s)
        else:
            # é‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢: sqrt(sum(w_i * (c_i - s_i)^2))
            return np.sqrt(np.sum(weights * (c - s)**2))
    elif metric == 'manhattan':
        if weights is None:
            return np.sum(np.abs(c - s))
        else:
            # é‡ã¿ä»˜ããƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢: sum(w_i * |c_i - s_i|)
            return np.sum(weights * np.abs(c - s))
    elif metric == 'cosine':
        if weights is None:
            return cosine_distances([c], [s])[0][0]
        else:
            # é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³è·é›¢: é‡ã¿ã‚’sqrt(w_i)ã§ç‰¹å¾´é‡ã«é©ç”¨ã—ã¦ã‹ã‚‰ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’è¨ˆç®—
            c_w = c * np.sqrt(weights)
            s_w = s * np.sqrt(weights)
            return cosine_distances([c_w], [s_w])[0][0]
    else:
        raise ValueError(f"æœªçŸ¥ã®è·é›¢é–¢æ•°ã§ã™: {metric}")

# --- K-means++ åˆæœŸåŒ– ---
def initialize_centroids(X_data, k):
    kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)
    kmeans.fit(X_data)
    return kmeans.cluster_centers_

# --- ä¸€èˆ¬çš„ãªK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ---
def general_kmeans_algorithm(X_data, k, metric='euclidean', weights=None, max_iterations=100):
    C = initialize_centroids(X_data, k)

    for iteration in range(max_iterations):
        # ã‚¹ãƒ†ãƒƒãƒ— 1: å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æœ€ã‚‚è¿‘ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦ã‚‹
        labels = np.zeros(len(X_data), dtype=int)
        for i, S in enumerate(X_data):
            dists = [dist(c, S, metric, weights=weights) for c in C]
            labels[i] = np.argmin(dists)

        # ã‚¹ãƒ†ãƒƒãƒ— 2: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‰²ã‚Šå½“ã¦ã«åŸºã¥ã„ã¦ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ›´æ–°
        new_C = np.zeros((k, X_data.shape[1]))
        for i in range(k):
            points_in_cluster = X_data[labels == i]
            if len(points_in_cluster) > 0:
                new_C[i] = np.mean(points_in_cluster, axis=0)
            else:
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒç©ºã«ãªã£ãŸå ´åˆã€ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ ã«å†åˆæœŸåŒ–ã™ã‚‹
                min_val = np.min(X_data, axis=0)
                max_val = np.max(X_data, axis=0)
                new_C[i] = np.random.uniform(min_val, max_val, X_data.shape[1])

        # åæŸåˆ¤å®š: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒã»ã¨ã‚“ã©å¤‰åŒ–ã—ãªããªã£ãŸã‚‰åœæ­¢
        if np.allclose(C, new_C):
            break

        C = new_C

    # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
    final_labels = np.zeros(len(X_data), dtype=int)
    for i, S in enumerate(X_data):
        dists = [dist(c, S, metric, weights=weights) for c in C]
        final_labels[i] = np.argmin(dists)

    return C, final_labels

# --- æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ---
def clustering_algorithm_with_correctness(X_data, k, is_correct_fn, metric='euclidean', weights=None, max_iterations=100):
    """
    æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

    Args:
        X_data: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        k: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
        is_correct_fn: æ­£è§£åˆ¤å®šé–¢æ•°
        metric: è·é›¢è¨ˆç®—æ–¹æ³•
        weights: ç‰¹å¾´é‡ã®é‡ã¿
        max_iterations: æœ€å¤§åå¾©å›æ•°

    Returns:
        C: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        final_labels: æœ€çµ‚ãƒ©ãƒ™ãƒ«
    """
    C = initialize_centroids(X_data, k)
    N = np.zeros(k)  # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®æ•°

    for S in X_data:
        # å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ S ã‚’æœ€ã‚‚è¿‘ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦ã‚‹
        dists = [dist(c, S, metric, weights=weights) for c in C]
        min_c = np.argmin(dists)  # å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        N[min_c] += 1

        # æ­£è§£åˆ¤å®šé–¢æ•°ãŒTrueã‚’è¿”ã—ãŸå ´åˆã«ã®ã¿ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ›´æ–°
        if is_correct_fn(S, min_c):
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ä¼¼ãŸã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ›´æ–°ï¼ˆ1ç‚¹ã”ã¨ã®ç§»å‹•å¹³å‡ï¼‰
            C[min_c] = C[min_c] + (1 / N[min_c]) * (S - C[min_c])

    # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
    final_labels = np.zeros(len(X_data), dtype=int)
    for i, S in enumerate(X_data):
        dists = [dist(c, S, metric, weights=weights) for c in C]
        final_labels[i] = np.argmin(dists)

    return C, final_labels

# --- æ­£è§£åˆ¤å®šé–¢æ•°ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ï¼ˆæ•™å¸«ã‚ã‚Šï¼‰ ---
def is_correct_fn_factory(true_centers):
    """
    çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’åŸºã«ã—ãŸæ­£è§£åˆ¤å®šé–¢æ•°ã‚’ç”Ÿæˆ

    Args:
        true_centers: çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã®é…åˆ—

    Returns:
        is_correct: æ­£è§£åˆ¤å®šé–¢æ•°
    """
    if true_centers is None:
        # çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒãŒãªã„å ´åˆã¯ã€å¸¸ã«Trueã‚’è¿”ã™
        print("Warning: No true_centers provided for correctness check. The algorithm will always consider an assignment 'correct'.")
        return lambda S, assigned_cluster_idx: True

    def is_correct(S, assigned_cluster_idx):
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ S ãŒã©ã®çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ã‹ã‚’åˆ¤æ–­
        true_dists = [np.linalg.norm(tc - S) for tc in true_centers]
        correct_cluster_idx = np.argmin(true_dists)

        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå‰²ã‚Šå½“ã¦ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒä¸€è‡´ã™ã‚‹ã‹ã©ã†ã‹ã‚’è¿”ã™
        return assigned_cluster_idx == correct_cluster_idx

    return is_correct

# --- JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’èª­ã¿è¾¼ã¿ ---
def load_true_centroids_from_cache(cache_file):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é‡å¿ƒï¼‰ã‚’èª­ã¿è¾¼ã¿

    Args:
        cache_file: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        true_centers: çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é…åˆ—
        pattern_labels: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆ
    """
    try:
        if FEATURE_EXTRACTION_AVAILABLE:
            cached_data = load_feature_vectors(cache_file)
        else:
            import json
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

        if cached_data and cached_data.get('pattern_centroids'):
            centroids_info = cached_data['pattern_centroids']

            if centroids_info and centroids_info.get('centroids'):
                pattern_centroids = []
                pattern_labels = []

                for pattern_name, centroid_info in centroids_info['centroids'].items():
                    pattern_centroids.append(centroid_info['centroid_vector'])
                    pattern_labels.append(pattern_name)

                true_centers = np.array(pattern_centroids)
                return true_centers, pattern_labels
            else:
                return None, None
        else:
            return None, None

    except Exception as e:
        return None, None

# --- ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’å‹•çš„ã«æ¤œå‡ºã™ã‚‹é–¢æ•° ---
def extract_pattern_from_filepath(filepath):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’å‹•çš„ã«æŠ½å‡º

    Args:
        filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        str: ãƒ‘ã‚¿ãƒ¼ãƒ³å (ä¾‹: "pattern4", "pattern5", "AC", "TLE", "other")
    """
    import re

    # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã«å¤‰æ›ï¼‰
    normalized_path = filepath.replace('\\', '/')

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚ç¢ºèª
    filename = os.path.basename(filepath)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹æ­£è¦è¡¨ç¾ã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    pattern_regexes = [
        # pattern + æ•°å­—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹å†…ï¼‰
        (r'pattern(\d+)', lambda m: f"pattern{m.group(1)}"),
        # AC, TLE ãªã©ã®çµæœãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ˜ç¢ºãªã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢åŒºåˆ‡ã‚Šï¼‰
        (r'_([A-Z]{2,3})(?:_|$|/|\.)', lambda m: m.group(1)),
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãŒçµæœã‚’è¡¨ã™å ´åˆ
        (r'/([A-Z]{2,3})/', lambda m: m.group(1)),
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•°å­—éƒ¨åˆ†ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦åˆ©ç”¨ï¼ˆsubmission_æ•°å­—.pyï¼‰
        (r'submission_(\d+)\.py', lambda m: f"sub{m.group(1)}"),
        # submissions_typical90_xx ãƒ‘ã‚¿ãƒ¼ãƒ³
        (r'submissions_typical90_([a-z]+)', lambda m: f"typical90_{m.group(1)}"),
        # ãã®ä»–ã®submissions_ãƒ‘ã‚¿ãƒ¼ãƒ³
        (r'submissions_([^/]+?)(?:_\d+)?/', lambda m: m.group(1) if not m.group(1).startswith('submission') else None),
    ]

    # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ãƒ‘ã‚¹ã§æ¤œç´¢
    for pattern_regex, extract_func in pattern_regexes:
        match = re.search(pattern_regex, normalized_path)
        if match:
            result = extract_func(match)
            if result:
                # ä¸€èˆ¬çš„ã§ãªã„å½¢å¼ã‚„çŸ­ã™ãã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
                if len(result) >= 2 and not result.isdigit():
                    return result

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã™ã‚‹æœ€å¾Œã®è©¦è¡Œ
    filename_patterns = [
        (r'^([a-z]+\d*)_', lambda m: m.group(1)),  # prefix_xxxå½¢å¼
        (r'_([a-z]+\d*)\.', lambda m: m.group(1)), # xxx_suffix.extå½¢å¼
        (r'(\d+)', lambda m: f"num{m.group(1)}"),  # æ•°å­—ã®ã¿ã®å ´åˆ
    ]

    for pattern_regex, extract_func in filename_patterns:
        match = re.search(pattern_regex, filename.lower())
        if match:
            result = extract_func(match)
            if result and len(result) >= 2:
                return result

    return "other"

def get_all_patterns_from_paths(file_paths):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‹ã‚‰å…¨ã¦ã®åˆ©ç”¨å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—

    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

    Returns:
        set: æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®é›†åˆ
    """
    patterns = set()
    for filepath in file_paths:
        pattern = extract_pattern_from_filepath(filepath)
        patterns.add(pattern)
    return patterns

# --- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜ ---
def save_clustering_results(final_labels, C_final, true_centers, file_names, file_paths,
                           algorithm_type, dataset_name, k_clusters, centroid_distance,
                           feature_vectors=None, output_dir=None):
    """
    ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        final_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ©ãƒ™ãƒ«
        C_final: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        true_centers: çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        file_names: ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        algorithm_type: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        k_clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
        centroid_distance: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢
        feature_vectors: ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        if output_dir is None:
            output_dir = f"clustering_results_{timestamp}"
            os.makedirs(output_dir, exist_ok=True)

        # çµæœã‚’æ•´ç†
        clustering_results = {
            "metadata": {
                "algorithm_type": algorithm_type,
                "dataset_name": dataset_name,
                "timestamp": timestamp,
                "k_clusters": k_clusters,
                "total_samples": len(final_labels),
                "centroid_distance": float(centroid_distance) if not np.isnan(centroid_distance) else None
            },
            "final_centroids": C_final.tolist() if C_final is not None else None,
            "true_centroids": true_centers.tolist() if true_centers is not None else None,
            "cluster_assignments": {},
            "cluster_statistics": {}
        }

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¥ã®è©³ç´°æƒ…å ±ã‚’ä½œæˆ
        unique_labels = np.unique(final_labels)
        for cluster_id in unique_labels:
            cluster_mask = final_labels == cluster_id
            cluster_indices = np.where(cluster_mask)[0]

            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            cluster_files = []
            if file_names and file_paths:
                for idx in cluster_indices:
                    file_info = {
                        "index": int(idx),
                        "filename": file_names[idx],
                        "filepath": file_paths[idx]
                    }

                    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
                    if feature_vectors is not None:
                        file_info["feature_vector"] = feature_vectors[idx].tolist()

                    # ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’å‹•çš„ã«æŠ½å‡º
                    filepath = file_paths[idx]
                    file_info["pattern"] = extract_pattern_from_filepath(filepath)

                    cluster_files.append(file_info)

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆæƒ…å ±
            cluster_stats = {
                "size": len(cluster_indices),
                "percentage": float(len(cluster_indices) / len(final_labels) * 100),
                "centroid": C_final[cluster_id].tolist() if C_final is not None else None
            }

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒçµ±è¨ˆ
            if file_paths:
                pattern_distribution = {}
                for file_info in cluster_files:
                    pattern = file_info.get("pattern", "other")
                    pattern_distribution[pattern] = pattern_distribution.get(pattern, 0) + 1
                cluster_stats["pattern_distribution"] = pattern_distribution

            clustering_results["cluster_assignments"][f"cluster_{cluster_id}"] = cluster_files
            clustering_results["cluster_statistics"][f"cluster_{cluster_id}"] = cluster_stats

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        filename = f"clustering_results_{algorithm_type}_{dataset_name}_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(clustering_results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")

        # çµ±è¨ˆæƒ…å ±ã‚’ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"ğŸ“Š ä¿å­˜ã•ã‚ŒãŸçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {algorithm_type}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {k_clusters}")
        print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(final_labels)}")
        if not np.isnan(centroid_distance):
            print(f"   ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢: {centroid_distance:.4f}")

        return filepath

    except Exception as e:
        print(f"âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

    #     C = new_C

    # # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
    # final_labels = np.zeros(len(X_data), dtype=int)
    # for i, S in enumerate(X_data):
    #     dists = [dist(c, S, metric, weights=weights) for c in C]
    #     final_labels[i] = np.argmin(dists)

    # return C, final_labels

# --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–¢æ•° ---
def create_dataset(dataset_name: str, n_samples: int = 300, target_directory: str = None, k_clusters: int = None):
    if dataset_name == 'real_code_features':
        # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
        if not FEATURE_EXTRACTION_AVAILABLE:
            raise ValueError("ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ext_cfg_dfg_feature.pyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã¾ãŸã¯å¯¾è©±å¼å…¥åŠ›ï¼‰
        if target_directory is None:
            # åˆ©ç”¨å¯èƒ½ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡º
            atcoder_base = "../atcoder"
            if os.path.exists(atcoder_base):
                available_dirs = []
                try:
                    for item in os.listdir(atcoder_base):
                        item_path = os.path.join(atcoder_base, item)
                        if os.path.isdir(item_path) and item.startswith("submissions_typical90_"):
                            available_dirs.append(item)

                    if available_dirs:
                        available_dirs.sort()
                        for i, dirname in enumerate(available_dirs, 1):
                            print(f"  {i}. {dirname}")

                        user_input = input("é¸æŠ (æ•°å­—ã¾ãŸã¯ãƒ‘ã‚¹): ").strip()

                        # æ•°å­—ã§ã®é¸æŠã®å ´åˆ
                        try:
                            choice_num = int(user_input)
                            if 1 <= choice_num <= len(available_dirs):
                                target_directory = os.path.join(atcoder_base, available_dirs[choice_num - 1])
                            else:
                                target_directory = user_input
                        except ValueError:
                            target_directory = user_input
                    else:
                        target_directory = input("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ç›´æ¥å…¥åŠ›: ").strip()
                except Exception:
                    target_directory = input("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ç›´æ¥å…¥åŠ›: ").strip()
            else:
                target_directory = input("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: ").strip()

            if not target_directory:
                target_directory = "../atcoder/submissions_typical90_d_15_AC_TLE"

        if not os.path.exists(target_directory):
            raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_directory}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        cache_file = f"feature_cache_{os.path.basename(target_directory)}.json"

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        code_files = find_files_in_directory(target_directory)

        if len(code_files) == 0:
            raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_directory}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ç”¨ï¼‰
        groups = analyze_file_groups(code_files, target_directory)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        batch_results = None
        use_cache = False

        if os.path.exists(cache_file):
            if check_cache_validity(target_directory, cache_file):
                use_cache = True
            else:
                use_cache = False
        else:
            use_cache = False

        if use_cache:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            cached_data = load_feature_vectors(cache_file)
            if cached_data:
                batch_results = cached_data['data']

        if batch_results is None:
            # æ–°è¦æŠ½å‡º
            batch_results = batch_extract_integrated_features(code_files)
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            save_feature_vectors(batch_results, groups=groups, base_directory=target_directory, output_file=cache_file, format='json')

        # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
        successful_results = [r for r in batch_results if 'error' not in r]

        if len(successful_results) == 0:
            raise ValueError("ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

        # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        X = np.array([r['integrated_vector'] for r in successful_results])

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’æŒ‡å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã¾ãŸã¯è‡ªå‹•æ±ºå®šï¼‰
        if k_clusters is None:
            while True:
                try:
                    k_input = input(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°K (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2, æ¨å¥¨ç¯„å›²: 2ï½{min(10, len(successful_results)//2)}): ").strip()
                    if not k_input:
                        k_clusters = 2
                        break

                    k_clusters = int(k_input)
                    if k_clusters < 2:
                        print("âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã¯2ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
                        continue
                    elif k_clusters > len(successful_results):
                        print(f"âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã¯ãƒ‡ãƒ¼ã‚¿æ•°({len(successful_results)})ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
                        continue
                    else:
                        break
                except ValueError:
                    print("âŒ æœ‰åŠ¹ãªæ•´æ•°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        n_features = 11

        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚ã€ä»®ã®ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
        y_true = np.zeros(len(successful_results))  # ã™ã¹ã¦åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã—ã¦æ‰±ã†

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿å­˜ï¼ˆå¾Œã§å‚ç…§ç”¨ï¼‰
        file_names = [os.path.basename(r['source_file']) for r in successful_results]

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†æç”¨ï¼‰
        file_paths = [r['source_file'] for r in successful_results]

        # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        true_centers, pattern_labels = load_true_centroids_from_cache(cache_file)

        # å¤–ã‚Œå€¤ï¼ˆotherãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ã®å‡¦ç†æ–¹é‡ç¢ºèª
        other_count = len([fp for fp in file_paths if extract_pattern_from_filepath(fp) == "other"])

        if true_centers is not None:
            # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‹ã‚‰'other'ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–ï¼ˆæ„å‘³ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰
            filtered_pattern_labels = []
            filtered_true_centers = []

            for i, label in enumerate(pattern_labels):
                if label != "other":  # 'other'ä»¥å¤–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã‚’ä½¿ç”¨
                    filtered_pattern_labels.append(label)
                    filtered_true_centers.append(true_centers[i])

            if filtered_true_centers:
                true_centers = np.array(filtered_true_centers)
                pattern_labels = filtered_pattern_labels
            else:
                true_centers = None

            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’ææ¡ˆ
            if true_centers is not None:
                suggested_k = len(true_centers)

                if k_clusters != suggested_k:
                    adjust_choice = input(f"æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {suggested_k} (æ„å‘³ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern_labels}). èª¿æ•´ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                    if adjust_choice in ['y', 'yes', '']:
                        k_clusters = suggested_k

        return X, y_true, k_clusters, n_features, true_centers, file_names, file_paths

    else:
        raise ValueError(f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã§ã™: {dataset_name}")

# --- æœ€çµ‚çš„ãªã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é–“ã®å¹³å‡æœ€å°è·é›¢ã‚’è¨ˆç®—ã™ã‚‹---
def calculate_average_min_centroid_distance(final_centroids, true_centers):
    if final_centroids is None or true_centers is None:
        return np.nan

    num_final = final_centroids.shape[0]
    num_true = true_centers.shape[0]

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒç•°ãªã‚‹å ´åˆã¯è­¦å‘Šï¼ˆãŸã ã—è¨ˆç®—ã¯ç¶šè¡Œï¼‰
    if num_final != num_true:
        print(f"Warning: Number of final centroids ({num_final}) does not match number of true centers ({num_true}). "
              "Distance calculation might be less meaningful.")

    min_distances = []
    for f_center in final_centroids:
        # å„æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«ã¤ã„ã¦ã€å…¨ã¦ã®çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨ã®è·é›¢ã‚’è¨ˆç®—
        distances_to_true = [np.linalg.norm(f_center - t_center) for t_center in true_centers]
        min_distances.append(np.min(distances_to_true))

    return np.mean(min_distances)

def display_clustering_results(final_labels, C_final, file_names=None, dataset_name="unknown", file_paths=None, feature_vectors=None):
    """
    ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆç°¡æ½”ç‰ˆï¼‰

    Args:
        final_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ©ãƒ™ãƒ«
        C_final: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        file_names: ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        feature_vectors: ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
    """
    print(f"\nğŸ“Š {dataset_name.upper()} ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ")
    print("=" * 80)

    unique_labels = np.unique(final_labels)
    print(f"ç·ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {len(unique_labels)} | ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(final_labels)}")

    for cluster_id in unique_labels:
        cluster_indices = np.where(final_labels == cluster_id)[0]
        cluster_size = len(cluster_indices)

        print(f"\nğŸ·ï¸ Cluster {cluster_id} ({cluster_size} ãƒ•ã‚¡ã‚¤ãƒ«):")

        if file_names and file_paths:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆ
            cluster_data = []
            for idx in cluster_indices:
                cluster_data.append({
                    'filename': file_names[idx],
                    'filepath': file_paths[idx],
                    'pattern': extract_pattern_from_filepath(file_paths[idx])
                })
            cluster_data.sort(key=lambda x: x['filename'])

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆ
            pattern_counts = {}
            for data in cluster_data:
                pattern = data['pattern']
                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆå€‹æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…%ï¼‰
            if pattern_counts:
                pattern_details = []
                for pattern, count in sorted(pattern_counts.items()):
                    pattern_details.append(f"{pattern}: {count}")
                pattern_info = ", ".join(pattern_details)
                print(f"   ğŸ“‚ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ: {pattern_info}")

                # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¡¨ç¤ºã‚’è¿½åŠ 
                percentage_details = []
                for pattern, count in sorted(pattern_counts.items()):
                    percentage = (count / cluster_size) * 100
                    percentage_details.append(f"{pattern}: {percentage:.4f}%")
                percentage_info = ", ".join(percentage_details)
                print(f"   ğŸ“Š ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸: {percentage_info}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°è¡¨ç¤ºï¼‰
            print(f"   ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
            for i, data in enumerate(cluster_data, 1):
                pattern_mark = "âš ï¸" if data['pattern'] == 'other' else ""
                print(f"       {i:2d}. {data['filename']:<25} ({data['pattern']}){pattern_mark}")

    print("=" * 80)

def main(algorithm_type: str, dataset_name: str, preloaded_data=None, target_directory: str = None, k_clusters: int = None):
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆã¾ãŸã¯äº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä½¿ç”¨
    if preloaded_data is None:
        result = create_dataset(dataset_name, target_directory=target_directory, k_clusters=k_clusters)
    else:
        result = preloaded_data

    # è¿”ã‚Šå€¤ã®æ•°ã«å¿œã˜ã¦é©åˆ‡ã«åˆ†å‰²
    if len(result) == 7:
        X, y_true, k_clusters, n_features, true_centers, file_names, file_paths = result
    elif len(result) == 6:
        X, y_true, k_clusters, n_features, true_centers, file_names = result
        file_paths = None
    else:
        X, y_true, k_clusters, n_features, true_centers = result
        file_names = None
        file_paths = None

    # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = f"clustering_results_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠã¨å®Ÿè¡Œ
    C_final, final_labels = None, None
    if algorithm_type == 'general':
        C_final, final_labels = general_kmeans_algorithm(
            X_data=X,  # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆå‰å‡¦ç†ãªã—ï¼‰
            k=k_clusters,
            metric='euclidean',
            weights=FEATURE_WEIGHTS if dataset_name == 'real_code_features' else None
        )
        algo_title = "General K-means"
    elif algorithm_type == 'correctness_guided':
        if true_centers is None:
            raise ValueError("æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã¯çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

        C_final, final_labels = clustering_algorithm_with_correctness(
            X_data=X,
            k=k_clusters,
            is_correct_fn=is_correct_fn_factory(true_centers),
            metric='euclidean',
            weights=FEATURE_WEIGHTS if dataset_name == 'real_code_features' else None
        )
        algo_title = "Correctness-Guided K-means"
    else:
        raise ValueError(f"ä¸æ˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—ã§ã™: {algorithm_type}. 'general' ã¾ãŸã¯ 'correctness_guided' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢ã®è¨ˆç®—
    centroid_distance = calculate_average_min_centroid_distance(C_final, true_centers)

    # çµæœã®å‡ºåŠ›
    print(f"\nğŸ“Š {dataset_name} - {algo_title} (k={k_clusters})")
    if true_centers is not None and not np.isnan(centroid_distance):
        print(f"ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢: {centroid_distance:.4f}")
    print("-" * 50)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜
    saved_file = save_clustering_results(
        final_labels=final_labels,
        C_final=C_final,
        true_centers=true_centers,
        file_names=file_names,
        file_paths=file_paths,
        algorithm_type=algorithm_type,
        dataset_name=dataset_name,
        k_clusters=k_clusters,
        centroid_distance=centroid_distance,
        feature_vectors=X,
        output_dir=output_dir
    )

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©³ç´°è¡¨ç¤º
    display_clustering_results(final_labels, C_final, file_names, dataset_name, file_paths, X)

    # å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯PCAã§æ¬¡å…ƒå‰Šæ¸›ï¼‰
    visualize_clustering_results(X, y_true, final_labels, C_final, true_centers,
                               dataset_name, algo_title, k_clusters, n_features, file_paths, output_dir)

    return saved_file, output_dir

def visualize_clustering_results(X, y_true, final_labels, C_final, true_centers,
                               dataset_name, algo_title, k_clusters, n_features, file_paths=None, output_dir=None):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘å¯¾å¿œï¼‰"""

    # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª1éšå±¤ï¼‰
    if output_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = f"clustering_results_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)

    # è¤‡æ•°ã®æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’è©¦è¡Œ
    reduction_results = {}

    if n_features > 2:
        # 1. PCA
        pca = PCA(n_components=2, random_state=42)
        X_pca = pca.fit_transform(X)
        C_pca = pca.transform(C_final)
        explained_var_ratio = pca.explained_variance_ratio_
        total_explained_var = np.sum(explained_var_ratio)

        reduction_results['PCA'] = {
            'X_2d': X_pca,
            'C_2d': C_pca,
            'title_suffix': f" (PCA 2D: {total_explained_var*100:.1f}% variance)",
            'info': f"PC1: {explained_var_ratio[0]*100:.1f}%, PC2: {explained_var_ratio[1]*100:.1f}%"
        }

        # 2. t-SNE
        if TSNE_AVAILABLE:
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X)-1), max_iter=1000)
            X_tsne = tsne.fit_transform(X)
            C_tsne = np.array([np.mean(X_tsne[final_labels == i], axis=0) for i in range(len(C_final))])

            reduction_results['t-SNE'] = {
                'X_2d': X_tsne,
                'C_2d': C_tsne,
                'title_suffix': f" (t-SNE 2D)",
                'info': f"perplexity: {min(30, len(X)-1)}, max_iter: 1000"
            }

        # 3. UMAP
        if UMAP_AVAILABLE:
            umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(X)-1))
            X_umap = umap_reducer.fit_transform(X)
            C_umap = np.array([np.mean(X_umap[final_labels == i], axis=0) for i in range(len(C_final))])

            reduction_results['UMAP'] = {
                'X_2d': X_umap,
                'C_2d': C_umap,
                'title_suffix': f" (UMAP 2D)",
                'info': f"n_neighbors: {min(15, len(X)-1)}"
            }
    else:
        # 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
        reduction_results['Original'] = {
            'X_2d': X,
            'C_2d': C_final,
            'title_suffix': "",
            'info': "Original 2D data"
        }

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®è‰²åˆ†ã‘æƒ…å ±ã‚’å–å¾—
    pattern_groups = None
    pattern_colors = None
    pattern_labels = None

    if file_paths is not None and dataset_name == 'real_code_features':
        # å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’ä½¿ç”¨
        all_patterns = get_all_patterns_from_paths(file_paths)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        pattern_groups = {}
        for pattern in all_patterns:
            pattern_groups[pattern] = []

        for i, filepath in enumerate(file_paths):
            pattern = extract_pattern_from_filepath(filepath)
            pattern_groups[pattern].append({
                'file_path': filepath,
                'index': i
            })

        # è‰²ã®è¨­å®š
        color_palette = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'olive', 'cyan']
        pattern_colors = {}
        color_idx = 0

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®è‰²å‰²ã‚Šå½“ã¦ï¼ˆ'other'ã‚‚é€šå¸¸ã®è‰²ã§è¡¨ç¤ºï¼‰
        for group_name in sorted(pattern_groups.keys()):
            if group_name == 'other':
                pattern_colors[group_name] = 'lightcoral'
            else:
                pattern_colors[group_name] = color_palette[color_idx % len(color_palette)]
                color_idx += 1

        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
        pattern_labels = [extract_pattern_from_filepath(fp) for fp in file_paths]

    # å„æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã”ã¨ã«å¯è¦–åŒ–ã‚’å®Ÿè¡Œ
    for method_name, result in reduction_results.items():
        X_2d = result['X_2d']
        C_final_2d = result['C_2d']
        title_suffix = result['title_suffix']
        method_info = result['info']

        # å›³ã‚’ä½œæˆ
        plt.figure(figsize=(18, 8))

        # å¯†é›†åº¦ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šã‚’èª¿æ•´
        n_points = len(X_2d)
        point_size = max(30, 100 - n_points // 10) if n_points > 100 else 60
        alpha_val = max(0.6, 1.0 - n_points / 500) if n_points > 100 else 0.8

        # å·¦å´: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘
        plt.subplot(1, 2, 1)
        if pattern_groups is not None:
            for group_name in pattern_groups.keys():
                group_indices = [i for i, label in enumerate(pattern_labels) if label == group_name]
                if group_indices:
                    group_points = X_2d[group_indices]
                    if group_name == 'other':
                        plt.scatter(group_points[:, 0], group_points[:, 1],
                                   c=pattern_colors[group_name],
                                   label=f'{group_name} (å¤–ã‚Œå€¤, {len(group_indices)})',
                                   alpha=alpha_val, s=point_size,
                                   edgecolors='black', linewidth=0.8, marker='^')
                    else:
                        plt.scatter(group_points[:, 0], group_points[:, 1],
                                   c=pattern_colors[group_name],
                                   label=f'{group_name} ({len(group_indices)})',
                                   alpha=alpha_val, s=point_size, edgecolors='black', linewidth=0.5)

            plt.title(f"Pattern-based Grouping ({method_name})", fontsize=12)
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        else:
            plt.scatter(X_2d[:, 0], X_2d[:, 1], c='gray', alpha=alpha_val, s=point_size)
            plt.title(f"Original Data ({method_name})")

        plt.xlabel(f"{method_name} Component 1" if n_features > 2 else "Feature 1")
        plt.ylabel(f"{method_name} Component 2" if n_features > 2 else "Feature 2")
        plt.grid(True, alpha=0.4)

        # å³å´: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
        plt.subplot(1, 2, 2)
        plt.scatter(X_2d[:, 0], X_2d[:, 1], c=final_labels, cmap='tab10',
                   alpha=alpha_val, s=point_size, edgecolors='black', linewidth=0.5)
        plt.title(f"{algo_title} Results ({method_name})", fontsize=12)

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        plt.scatter(C_final_2d[:, 0], C_final_2d[:, 1],
                   c='red', s=250, marker='X', edgecolor='black', linewidth=2, alpha=1.0)

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆã‚’å‡¡ä¾‹ã¨ã—ã¦è¡¨ç¤º
        unique_clusters = np.unique(final_labels)
        legend_elements = []
        for cluster_id in unique_clusters:
            cluster_count = np.sum(final_labels == cluster_id)
            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w',
                                            markerfacecolor=cm.get_cmap('tab10')(cluster_id / 10.0), markersize=8,
                                            label=f'C{cluster_id} ({cluster_count})'))
        legend_elements.append(plt.Line2D([0], [0], marker='X', color='w',
                                        markerfacecolor='red', markersize=12, markeredgecolor='black',
                                        label='Centroids'))
        plt.legend(handles=legend_elements, loc='upper right', fontsize=9)

        plt.xlabel(f"{method_name} Component 1" if n_features > 2 else "Feature 1")
        plt.ylabel(f"{method_name} Component 2" if n_features > 2 else "Feature 2")
        plt.grid(True, alpha=0.4)

        plt.tight_layout()

        # ç”»åƒã¨ã—ã¦ä¿å­˜
        method_filename = method_name.lower().replace('-', '_')
        timestamp = output_dir.split('_')[-1] if 'clustering_results_' in output_dir else datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = os.path.join(output_dir, f"clustering_result_{dataset_name}_{algo_title.lower().replace(' ', '_').replace('-', '_')}_{method_filename}_{timestamp}.png")
        plt.savefig(filename, dpi=200, bbox_inches='tight')

        print(f"\nğŸ“ˆ {method_name}å¯è¦–åŒ–çµæœã‚’ '{filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

        plt.show()

if __name__ == '__main__':

    # --- å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ---
    if FEATURE_EXTRACTION_AVAILABLE:
        # å®Ÿè¡Œå±¥æ­´ã‚’è¿½è·¡
        executed_directories = []
        all_saved_files = []

        # é€£ç¶šå®Ÿè¡Œãƒ«ãƒ¼ãƒ—
        while True:
            saved_files = []
            shared_data = None

            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
                shared_data = create_dataset('real_code_features')
            except Exception as e:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                shared_data = None

            if shared_data is not None:
                # å®Ÿè¡Œã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨˜éŒ²
                if len(shared_data) >= 7:
                    current_dir = os.path.basename(os.path.dirname(shared_data[6][0])) if shared_data[6] else "unknown"
                    if current_dir not in executed_directories:
                        executed_directories.append(current_dir)

                try:
                    # 1. ä¸€èˆ¬çš„ãªK-meansã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                    general_result_file, general_output_dir = main(algorithm_type='general', dataset_name='real_code_features', preloaded_data=shared_data)
                    saved_files.append(('general', general_result_file, general_output_dir))
                    all_saved_files.append(('general', general_result_file, general_output_dir, current_dir))
                except Exception as e:
                    print(f"âŒ ä¸€èˆ¬çš„ãªK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

                try:
                    # 2. æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
                    correctness_result_file, correctness_output_dir = main(algorithm_type='correctness_guided', dataset_name='real_code_features', preloaded_data=shared_data)
                    saved_files.append(('correctness_guided', correctness_result_file, correctness_output_dir))
                    all_saved_files.append(('correctness_guided', correctness_result_file, correctness_output_dir, current_dir))
                except Exception as e:
                    print(f"âŒ æ­£è§£åˆ¤å®šé–¢æ•°ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

                # çµæœã‚µãƒãƒªãƒ¼
                print(f"\n{'='*60}")
                print("ğŸ‰ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
                if saved_files:
                    for algorithm_type, result_file, output_dir in saved_files:
                        if result_file:
                            print(f"âœ… {algorithm_type.upper()}: {os.path.basename(result_file)}")
                        else:
                            print(f"âŒ {algorithm_type.upper()}: ä¿å­˜å¤±æ•—")
                print(f"{'='*60}")

            # æ¬¡ã®å®Ÿè¡Œé¸æŠ
            atcoder_base = "../atcoder"
            available_dirs = []
            if os.path.exists(atcoder_base):
                try:
                    for item in os.listdir(atcoder_base):
                        item_path = os.path.join(atcoder_base, item)
                        if os.path.isdir(item_path) and item.startswith("submissions_typical90_"):
                            available_dirs.append(item)
                    available_dirs.sort()
                except Exception:
                    pass

            # é¸æŠè‚¢ã‚’è¡¨ç¤º
            if available_dirs:
                print(f"\nåˆ©ç”¨å¯èƒ½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:")
                for i, dirname in enumerate(available_dirs, 1):
                    status = " (âœ…)" if dirname in executed_directories else ""
                    print(f"  {i}. {dirname}{status}")
                print(f"  0. çµ‚äº†")

                choice = input(f"\né¸æŠ (0-{len(available_dirs)}): ").strip()

                if choice == "0" or choice.lower() in ['exit', 'quit', 'q']:
                    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
                    if all_saved_files:
                        print(f"\n{'='*80}")
                        print("ğŸŠ å…¨å®Ÿè¡Œã‚µãƒãƒªãƒ¼")
                        print(f"{'='*80}")
                        print(f"ğŸ“Š å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(executed_directories)}")
                        print(f"ğŸ“ å®Ÿè¡Œæ¸ˆã¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {', '.join(executed_directories)}")
                        print(f"ï¿½ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_saved_files)}")

                        print(f"\nğŸ“‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ¥çµæœ:")
                        current_dir_files = {}
                        for algo_type, result_file, output_dir, dir_name in all_saved_files:
                            if dir_name not in current_dir_files:
                                current_dir_files[dir_name] = []
                            current_dir_files[dir_name].append((algo_type, result_file, output_dir))

                        for dir_name, files in current_dir_files.items():
                            print(f"   ğŸ“ {dir_name}:")
                            for algo_type, result_file, output_dir in files:
                                if result_file:
                                    print(f"      âœ… {algo_type}: {os.path.basename(result_file)}")
                                else:
                                    print(f"      âŒ {algo_type}: ä¿å­˜å¤±æ•—")
                        print(f"{'='*80}")

                    print("ï¿½ğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                elif choice.isdigit():
                    choice_num = int(choice)
                    if 1 <= choice_num <= len(available_dirs):
                        # é¸æŠã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§æ¬¡å›å®Ÿè¡Œ
                        selected_dir = available_dirs[choice_num - 1]
                        if selected_dir in executed_directories:
                            confirm = input(f"âš ï¸ {selected_dir} ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ã§ã™ã€‚å†å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                            if confirm not in ['y', 'yes', '']:
                                continue
                        print(f"ğŸ“ æ¬¡å›å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {selected_dir}")
                        continue
                    elif choice_num == len(available_dirs) + 1:
                        print("ğŸ“ åˆ¥ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æ¬¡å›å…¥åŠ›ã§æŒ‡å®šã§ãã¾ã™ã€‚")
                        continue
                    else:
                        print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                        break
                else:
                    print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                    break
            else:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                print("ğŸ“‚ åˆ©ç”¨å¯èƒ½ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                continue_choice = input("åˆ¥ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                if continue_choice not in ['y', 'yes', '']:
                    print("ğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break

    else:
        print("âš ï¸ ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
