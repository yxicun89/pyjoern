# ã“ã‚ŒãŒç¾æ™‚ç‚¹ã§ã®å®Œæˆç‰ˆ
# 11æ¬¡å…ƒã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_distances
from sklearn.datasets import make_blobs, make_circles, make_moons, load_iris, load_wine
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import os
from datetime import datetime

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import seaborn as sns
    import pandas as pd
    ADVANCED_VIZ_AVAILABLE = True
except ImportError:
    ADVANCED_VIZ_AVAILABLE = False
    print("âš ï¸ seaborn/pandasãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªå¯è¦–åŒ–ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
    print("   é«˜åº¦ãªå¯è¦–åŒ–ã«ã¯: pip install seaborn pandas ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# JSONãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json

# æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sklearn.manifold import TSNE
    TSNE_AVAILABLE = True
except ImportError:
    TSNE_AVAILABLE = False
    print("âš ï¸ t-SNEãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚scikit-learnã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("âš ï¸ UMAPãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«ã¯: pip install umap-learn ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ext_cfg_dfg_feature.pyã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from ext_cfg_dfg_feature import (
        extract_integrated_features_vector,
        batch_extract_integrated_features,
        find_files_in_directory,
        load_feature_vectors,
        save_feature_vectors,
        check_cache_validity,
        analyze_file_groups
    )
    FEATURE_EXTRACTION_AVAILABLE = True
except ImportError as e:
    print(f"âŒ ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ext_cfg_dfg_feature.pyãŒåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    FEATURE_EXTRACTION_AVAILABLE = False

# --- ç‰¹å¾´é‡ã®é‡ã¿ã‚’å®šç¾© ---
# connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity
# variable_count, total_reads, total_writes, max_reads, max_writes ã«å¯¾å¿œ
FEATURE_WEIGHTS = np.array([
    1.0, # connected_components
    1.0, # loop_statements
    1.0, # conditional_statements
    1.0, # cycles
    1.0, # paths
    1.0, # cyclomatic_complexity
    0.6, # variable_count
    0.1, # total_reads
    0.1, # total_writes
    0.1, # max_reads
    0.1  # max_writes
])

# --- è·é›¢é–¢æ•°ï¼ˆé‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã€ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã€ã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰ ---
def dist(c, s, metric='euclidean', weights=None):
    if metric == 'euclidean':
        if weights is None:
            return np.linalg.norm(c - s)
        else:
            # é‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢: sqrt(sum(w_i * (c_i - s_i)^2))
            return np.sqrt(np.sum(weights * (c - s)**2))
    elif metric == 'manhattan':
        if weights is None:
            return np.sum(np.abs(c - s))
        else:
            # é‡ã¿ä»˜ããƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢: sum(w_i * |c_i - s_i|)
            return np.sum(weights * np.abs(c - s))
    elif metric == 'cosine':
        if weights is None:
            return cosine_distances([c], [s])[0][0]
        else:
            # é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³è·é›¢: é‡ã¿ã‚’sqrt(w_i)ã§ç‰¹å¾´é‡ã«é©ç”¨ã—ã¦ã‹ã‚‰ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’è¨ˆç®—
            c_w = c * np.sqrt(weights)
            s_w = s * np.sqrt(weights)
            return cosine_distances([c_w], [s_w])[0][0]
    else:
        raise ValueError(f"æœªçŸ¥ã®è·é›¢é–¢æ•°ã§ã™: {metric}")

# --- K-means++ åˆæœŸåŒ– ---
def initialize_centroids(X_data, k):
    kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)
    kmeans.fit(X_data)
    return kmeans.cluster_centers_

# --- ä¸€èˆ¬çš„ãªK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ---
def general_kmeans_algorithm(X_data, k, metric='euclidean', weights=None, max_iterations=100):
    C = initialize_centroids(X_data, k)

    for iteration in range(max_iterations):
        # ã‚¹ãƒ†ãƒƒãƒ— 1: å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æœ€ã‚‚è¿‘ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦ã‚‹
        labels = np.zeros(len(X_data), dtype=int)
        for i, S in enumerate(X_data):
            dists = [dist(c, S, metric, weights=weights) for c in C]
            labels[i] = np.argmin(dists)

        # ã‚¹ãƒ†ãƒƒãƒ— 2: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å‰²ã‚Šå½“ã¦ã«åŸºã¥ã„ã¦ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ›´æ–°
        new_C = np.zeros((k, X_data.shape[1]))
        for i in range(k):
            points_in_cluster = X_data[labels == i]
            if len(points_in_cluster) > 0:
                new_C[i] = np.mean(points_in_cluster, axis=0)
            else:
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒç©ºã«ãªã£ãŸå ´åˆã€ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ç¯„å›²å†…ã§ãƒ©ãƒ³ãƒ€ãƒ ã«å†åˆæœŸåŒ–ã™ã‚‹
                min_val = np.min(X_data, axis=0)
                max_val = np.max(X_data, axis=0)
                new_C[i] = np.random.uniform(min_val, max_val, X_data.shape[1])

        # åæŸåˆ¤å®š: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒã»ã¨ã‚“ã©å¤‰åŒ–ã—ãªããªã£ãŸã‚‰åœæ­¢
        if np.allclose(C, new_C):
            break

        C = new_C

    # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
    final_labels = np.zeros(len(X_data), dtype=int)
    for i, S in enumerate(X_data):
        dists = [dist(c, S, metric, weights=weights) for c in C]
        final_labels[i] = np.argmin(dists)

    return C, final_labels

# --- æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ---
def clustering_algorithm_with_correctness(X_data, k, is_correct_fn, metric='euclidean', weights=None, max_iterations=100):
    """
    æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

    Args:
        X_data: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        k: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
        is_correct_fn: æ­£è§£åˆ¤å®šé–¢æ•°
        metric: è·é›¢è¨ˆç®—æ–¹æ³•
        weights: ç‰¹å¾´é‡ã®é‡ã¿
        max_iterations: æœ€å¤§åå¾©å›æ•°

    Returns:
        C: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        final_labels: æœ€çµ‚ãƒ©ãƒ™ãƒ«
    """
    C = initialize_centroids(X_data, k)
    N = np.zeros(k)  # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®æ•°

    for S in X_data:
        # å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ S ã‚’æœ€ã‚‚è¿‘ã„ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å‰²ã‚Šå½“ã¦ã‚‹
        dists = [dist(c, S, metric, weights=weights) for c in C]
        min_c = np.argmin(dists)  # å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        N[min_c] += 1

        # æ­£è§£åˆ¤å®šé–¢æ•°ãŒTrueã‚’è¿”ã—ãŸå ´åˆã«ã®ã¿ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ›´æ–°
        if is_correct_fn(S, min_c):
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ä¼¼ãŸã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ›´æ–°ï¼ˆ1ç‚¹ã”ã¨ã®ç§»å‹•å¹³å‡ï¼‰
            C[min_c] = C[min_c] + (1 / N[min_c]) * (S - C[min_c])

    # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
    final_labels = np.zeros(len(X_data), dtype=int)
    for i, S in enumerate(X_data):
        dists = [dist(c, S, metric, weights=weights) for c in C]
        final_labels[i] = np.argmin(dists)

    return C, final_labels

# --- æ­£è§£åˆ¤å®šé–¢æ•°ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ï¼ˆæ•™å¸«ã‚ã‚Šï¼‰ ---
def is_correct_fn_factory(true_centers):
    """
    çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’åŸºã«ã—ãŸæ­£è§£åˆ¤å®šé–¢æ•°ã‚’ç”Ÿæˆ

    Args:
        true_centers: çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã®é…åˆ—

    Returns:
        is_correct: æ­£è§£åˆ¤å®šé–¢æ•°
    """
    if true_centers is None:
        # çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒãŒãªã„å ´åˆã¯ã€å¸¸ã«Trueã‚’è¿”ã™
        print("Warning: No true_centers provided for correctness check. The algorithm will always consider an assignment 'correct'.")
        return lambda S, assigned_cluster_idx: True

    def is_correct(S, assigned_cluster_idx):
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ S ãŒã©ã®çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ã‹ã‚’åˆ¤æ–­
        true_dists = [np.linalg.norm(tc - S) for tc in true_centers]
        correct_cluster_idx = np.argmin(true_dists)

        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå‰²ã‚Šå½“ã¦ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨çœŸã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒä¸€è‡´ã™ã‚‹ã‹ã©ã†ã‹ã‚’è¿”ã™
        return assigned_cluster_idx == correct_cluster_idx

    return is_correct

# --- JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’èª­ã¿è¾¼ã¿ ---
def load_true_centroids_from_cache(cache_file):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é‡å¿ƒï¼‰ã‚’èª­ã¿è¾¼ã¿

    Args:
        cache_file: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        true_centers: çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é…åˆ—
        pattern_labels: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆ
    """
    try:
        if FEATURE_EXTRACTION_AVAILABLE:
            cached_data = load_feature_vectors(cache_file)
        else:
            import json
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

        if cached_data and cached_data.get('pattern_centroids'):
            centroids_info = cached_data['pattern_centroids']

            if centroids_info and centroids_info.get('centroids'):
                pattern_centroids = []
                pattern_labels = []

                for pattern_name, centroid_info in centroids_info['centroids'].items():
                    pattern_centroids.append(centroid_info['centroid_vector'])
                    pattern_labels.append(pattern_name)

                true_centers = np.array(pattern_centroids)

                print(f"âœ… çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ:")
                print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(pattern_labels)}")
                print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {true_centers.shape[1]}")
                for i, label in enumerate(pattern_labels):
                    print(f"   {label}: {np.round(true_centers[i][:3], 3)}...")

                return true_centers, pattern_labels
            else:
                print("âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
                return None, None
        else:
            print("âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
            return None, None

    except Exception as e:
        print(f"âŒ çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

# --- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜ ---
def save_clustering_results(final_labels, C_final, true_centers, file_names, file_paths,
                           algorithm_type, dataset_name, k_clusters, centroid_distance,
                           feature_vectors=None, output_dir=None):
    """
    ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        final_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ©ãƒ™ãƒ«
        C_final: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        true_centers: çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        file_names: ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        algorithm_type: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        k_clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
        centroid_distance: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢
        feature_vectors: ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        if output_dir is None:
            output_dir = f"clustering_results_{timestamp}"
            os.makedirs(output_dir, exist_ok=True)

        # çµæœã‚’æ•´ç†
        clustering_results = {
            "metadata": {
                "algorithm_type": algorithm_type,
                "dataset_name": dataset_name,
                "timestamp": timestamp,
                "k_clusters": k_clusters,
                "total_samples": len(final_labels),
                "centroid_distance": float(centroid_distance) if not np.isnan(centroid_distance) else None
            },
            "final_centroids": C_final.tolist() if C_final is not None else None,
            "true_centroids": true_centers.tolist() if true_centers is not None else None,
            "cluster_assignments": {},
            "cluster_statistics": {}
        }

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¥ã®è©³ç´°æƒ…å ±ã‚’ä½œæˆ
        unique_labels = np.unique(final_labels)
        for cluster_id in unique_labels:
            cluster_mask = final_labels == cluster_id
            cluster_indices = np.where(cluster_mask)[0]

            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            cluster_files = []
            if file_names and file_paths:
                for idx in cluster_indices:
                    file_info = {
                        "index": int(idx),
                        "filename": file_names[idx],
                        "filepath": file_paths[idx]
                    }

                    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
                    if feature_vectors is not None:
                        file_info["feature_vector"] = feature_vectors[idx].tolist()

                    # ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’æŠ½å‡º
                    filepath = file_paths[idx]
                    if 'pattern1' in filepath:
                        file_info["pattern"] = "pattern1"
                    elif 'pattern2' in filepath:
                        file_info["pattern"] = "pattern2"
                    elif 'pattern3' in filepath:
                        file_info["pattern"] = "pattern3"
                    elif 'pattern4' in filepath:
                        file_info["pattern"] = "pattern4"
                    else:
                        file_info["pattern"] = "other"

                    cluster_files.append(file_info)

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆæƒ…å ±
            cluster_stats = {
                "size": len(cluster_indices),
                "percentage": float(len(cluster_indices) / len(final_labels) * 100),
                "centroid": C_final[cluster_id].tolist() if C_final is not None else None
            }

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒçµ±è¨ˆ
            if file_paths:
                pattern_distribution = {}
                for file_info in cluster_files:
                    pattern = file_info.get("pattern", "other")
                    pattern_distribution[pattern] = pattern_distribution.get(pattern, 0) + 1
                cluster_stats["pattern_distribution"] = pattern_distribution

            clustering_results["cluster_assignments"][f"cluster_{cluster_id}"] = cluster_files
            clustering_results["cluster_statistics"][f"cluster_{cluster_id}"] = cluster_stats

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        filename = f"clustering_results_{algorithm_type}_{dataset_name}_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(clustering_results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")

        # çµ±è¨ˆæƒ…å ±ã‚’ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"ğŸ“Š ä¿å­˜ã•ã‚ŒãŸçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {algorithm_type}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {k_clusters}")
        print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(final_labels)}")
        if not np.isnan(centroid_distance):
            print(f"   ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢: {centroid_distance:.4f}")

        return filepath

    except Exception as e:
        print(f"âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

        C = new_C

    # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«ä»˜ã‘
    final_labels = np.zeros(len(X_data), dtype=int)
    for i, S in enumerate(X_data):
        dists = [dist(c, S, metric, weights=weights) for c in C]
        final_labels[i] = np.argmin(dists)

    return C, final_labels

# --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–¢æ•° ---
def create_dataset(dataset_name: str, n_samples: int = 300):
    if dataset_name == 'real_code_features':
        # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
        if not FEATURE_EXTRACTION_AVAILABLE:
            raise ValueError("ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ext_cfg_dfg_feature.pyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã¾ãŸã¯çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
        target_directory = "../atcoder/submissions_typical90_d_100"

        if not os.path.exists(target_directory):
            raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_directory}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        cache_file = f"feature_cache_{os.path.basename(target_directory)}.json"

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        code_files = find_files_in_directory(target_directory)

        if len(code_files) == 0:
            raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_directory}")

        print(f"ğŸ” ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(code_files)}")
        for i, file in enumerate(code_files[:5]):  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
            print(f"  {i+1}. {os.path.relpath(file, target_directory)}")
        if len(code_files) > 5:
            print(f"  ... ãŠã‚ˆã³ {len(code_files) - 5} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        batch_results = None
        use_cache = False

        if os.path.exists(cache_file):
            if check_cache_validity(target_directory, cache_file):
                print(f"ğŸ“¦ æœ‰åŠ¹ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {cache_file}")
                print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                use_cache = True
            else:
                print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¤ã„ãŸã‚ã€å†æŠ½å‡ºãŒå¿…è¦ã§ã™")

        if use_cache:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            print(f"ğŸ“‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            cached_data = load_feature_vectors(cache_file)
            if cached_data:
                batch_results = cached_data['data']
                print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ {len(batch_results)} ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

        if batch_results is None:
            # æ–°è¦æŠ½å‡º
            print("ğŸ“Š ç‰¹å¾´é‡æŠ½å‡ºä¸­...")
            batch_results = batch_extract_integrated_features(code_files)

            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            print(f"ğŸ’¾ ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ä¸­...")
            save_feature_vectors(batch_results, cache_file, format='json')

        # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
        successful_results = [r for r in batch_results if 'error' not in r]

        if len(successful_results) == 0:
            raise ValueError("ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

        print(f"âœ… ç‰¹å¾´é‡æŠ½å‡ºæˆåŠŸ: {len(successful_results)} / {len(code_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

        # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        X = np.array([r['integrated_vector'] for r in successful_results])

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’è‡ªå‹•æ±ºå®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«åŸºã¥ãï¼‰
        k_clusters = 5
        n_features = 11

        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯çœŸã®ãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚ã€ä»®ã®ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
        y_true = np.zeros(len(successful_results))  # ã™ã¹ã¦åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã—ã¦æ‰±ã†

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿å­˜ï¼ˆå¾Œã§å‚ç…§ç”¨ï¼‰
        file_names = [os.path.basename(r['source_file']) for r in successful_results]

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†æç”¨ï¼‰
        file_paths = [r['source_file'] for r in successful_results]

        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(X)} ã‚µãƒ³ãƒ—ãƒ«, {n_features} ç‰¹å¾´é‡, {k_clusters} ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")

        # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        true_centers, pattern_labels = load_true_centroids_from_cache(cache_file)

        if true_centers is not None:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’èª¿æ•´
            k_clusters = len(true_centers)
            print(f"ğŸ¯ çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’èª¿æ•´: {k_clusters}")

        # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹æƒ…å ±ã‚’è¿”ã‚Šå€¤ã«å«ã‚ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        return X, y_true, k_clusters, n_features, true_centers, file_names, file_paths

    else:
        raise ValueError(f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã§ã™: {dataset_name}")

# --- æœ€çµ‚çš„ãªã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é–“ã®å¹³å‡æœ€å°è·é›¢ã‚’è¨ˆç®—ã™ã‚‹---
def calculate_average_min_centroid_distance(final_centroids, true_centers):
    if final_centroids is None or true_centers is None:
        return np.nan

    num_final = final_centroids.shape[0]
    num_true = true_centers.shape[0]

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒç•°ãªã‚‹å ´åˆã¯è­¦å‘Šï¼ˆãŸã ã—è¨ˆç®—ã¯ç¶šè¡Œï¼‰
    if num_final != num_true:
        print(f"Warning: Number of final centroids ({num_final}) does not match number of true centers ({num_true}). "
              "Distance calculation might be less meaningful.")

    min_distances = []
    for f_center in final_centroids:
        # å„æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«ã¤ã„ã¦ã€å…¨ã¦ã®çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨ã®è·é›¢ã‚’è¨ˆç®—
        distances_to_true = [np.linalg.norm(f_center - t_center) for t_center in true_centers]
        min_distances.append(np.min(distances_to_true))

    return np.mean(min_distances)

def display_clustering_results(final_labels, C_final, file_names=None, dataset_name="unknown", file_paths=None, feature_vectors=None):
    """
    ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’è©³ç´°è¡¨ç¤º

    Args:
        final_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ©ãƒ™ãƒ«
        C_final: æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰
        file_names: ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        feature_vectors: ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
    """
    print(f"\nğŸ“Š === {dataset_name.upper()} ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœè©³ç´° ===")

    unique_labels = np.unique(final_labels)
    print(f"ğŸ”¢ ç·ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°: {len(unique_labels)}")
    print(f"ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(final_labels)}")

    print(f"\nğŸ¯ å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è©³ç´°:")
    print("=" * 100)

    for cluster_id in unique_labels:
        cluster_indices = np.where(final_labels == cluster_id)[0]
        cluster_size = len(cluster_indices)

        print(f"\nğŸ·ï¸  ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster_id}:")
        print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {cluster_size} ã‚µãƒ³ãƒ—ãƒ« ({cluster_size/len(final_labels)*100:.1f}%)")
        print(f"   ğŸ¯ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰: {np.round(C_final[cluster_id], 3)}")

        # ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        if file_names and file_paths and feature_vectors is not None:
            print(f"   ğŸ“„ å«ã¾ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°:")
            print(f"   {'No':<3} {'ãƒ•ã‚¡ã‚¤ãƒ«å':<25} {'ãƒ‘ã‚¹':<50} {'ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«'}")
            print(f"   {'-'*3} {'-'*25} {'-'*50} {'-'*50}")

            cluster_data = []
            for idx in cluster_indices:
                cluster_data.append({
                    'index': idx,
                    'filename': file_names[idx],
                    'filepath': file_paths[idx] if file_paths else 'N/A',
                    'vector': feature_vectors[idx] if feature_vectors is not None else 'N/A'
                })

            # ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆ
            cluster_data.sort(key=lambda x: x['filename'])

            for i, data in enumerate(cluster_data, 1):
                filename = data['filename']
                filepath = data['filepath']
                vector = data['vector']

                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã‚’æŠ½å‡º
                if filepath != 'N/A':
                    path_parts = filepath.split('/')
                    if len(path_parts) > 2:
                        short_path = '/'.join(path_parts[-2:])  # æœ€å¾Œã®2éšå±¤ã®ã¿è¡¨ç¤º
                    else:
                        short_path = filepath
                else:
                    short_path = 'N/A'

                # ãƒ™ã‚¯ãƒˆãƒ«ã‚’çŸ­ç¸®è¡¨ç¤º
                if isinstance(vector, (list, np.ndarray)):
                    vector_str = str(vector).replace(' ', '')[1:-1]  # ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ã€[]å‰Šé™¤
                    if len(vector_str) > 50:
                        vector_str = vector_str[:47] + "..."
                else:
                    vector_str = str(vector)

                print(f"   {i:2d}. {filename:<25} {short_path:<50} [{vector_str}]")

                # 10å€‹ä»¥ä¸Šã‚ã‚‹å ´åˆã¯çœç•¥è¡¨ç¤º
                if i >= 10 and len(cluster_data) > 10:
                    remaining = len(cluster_data) - 10
                    print(f"       ... ãŠã‚ˆã³ {remaining} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
                    break

        elif file_names:
            print(f"   ğŸ“„ å«ã¾ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:")
            cluster_files = [file_names[idx] for idx in cluster_indices]

            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
            cluster_files.sort()
            for i, filename in enumerate(cluster_files, 1):
                print(f"      {i:2d}. {filename}")
                if i >= 10 and len(cluster_files) > 10:  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤º
                    remaining = len(cluster_files) - 10
                    print(f"      ... ãŠã‚ˆã³ {remaining} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
                    break
        else:
            print(f"   ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {cluster_indices[:10].tolist()}" +
                  (f" ... (+{len(cluster_indices)-10})" if len(cluster_indices) > 10 else ""))

    print("=" * 100)

def main(algorithm_type: str, dataset_name: str):
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
    result = create_dataset(dataset_name)

    # è¿”ã‚Šå€¤ã®æ•°ã«å¿œã˜ã¦é©åˆ‡ã«åˆ†å‰²
    if len(result) == 7:
        X, y_true, k_clusters, n_features, true_centers, file_names, file_paths = result
    elif len(result) == 6:
        X, y_true, k_clusters, n_features, true_centers, file_names = result
        file_paths = None
    else:
        X, y_true, k_clusters, n_features, true_centers = result
        file_names = None
        file_paths = None

    # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = f"clustering_results_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠã¨å®Ÿè¡Œ
    C_final, final_labels = None, None
    if algorithm_type == 'general':
        C_final, final_labels = general_kmeans_algorithm(
            X_data=X,  # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆå‰å‡¦ç†ãªã—ï¼‰
            k=k_clusters,
            metric='euclidean',
            weights=FEATURE_WEIGHTS if dataset_name == 'real_code_features' else None
        )
        algo_title = "General K-means"
    elif algorithm_type == 'correctness_guided':
        if true_centers is None:
            print("âŒ æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã¯çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")
            print("   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            raise ValueError("çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ext_cfg_dfg_feature.pyã‚’å®Ÿè¡Œã—ã¦ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")

        print(f"ğŸ¯ æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ (çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ•°: {len(true_centers)})")
        C_final, final_labels = clustering_algorithm_with_correctness(
            X_data=X,
            k=k_clusters,
            is_correct_fn=is_correct_fn_factory(true_centers),
            metric='euclidean',
            weights=FEATURE_WEIGHTS if dataset_name == 'real_code_features' else None
        )
        algo_title = "Correctness-Guided K-means"
    else:
        raise ValueError(f"ä¸æ˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—ã§ã™: {algorithm_type}. 'general' ã¾ãŸã¯ 'correctness_guided' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢ã®è¨ˆç®—
    centroid_distance = calculate_average_min_centroid_distance(C_final, true_centers)

    # çµæœã®å‡ºåŠ›
    print(f"--- {dataset_name.capitalize()} Dataset Results ({algo_title}, k={k_clusters}) ---")
    print(f"æœ€çµ‚çš„ãªã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰:\n", np.round(C_final, 2))
    if true_centers is not None and not np.isnan(centroid_distance):
        print(f"æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰é–“ã®å¹³å‡æœ€å°è·é›¢: {centroid_distance:.4f}")
    else:
        print("çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è·é›¢ã¯è¨ˆç®—ã•ã‚Œã¾ã›ã‚“ã€‚")
    print("-" * 50)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜
    saved_file = save_clustering_results(
        final_labels=final_labels,
        C_final=C_final,
        true_centers=true_centers,
        file_names=file_names,
        file_paths=file_paths,
        algorithm_type=algorithm_type,
        dataset_name=dataset_name,
        k_clusters=k_clusters,
        centroid_distance=centroid_distance,
        feature_vectors=X,
        output_dir=output_dir
    )

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©³ç´°è¡¨ç¤º
    display_clustering_results(final_labels, C_final, file_names, dataset_name, file_paths, X)

    # å¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯PCAã§æ¬¡å…ƒå‰Šæ¸›ï¼‰
    visualize_clustering_results(X, y_true, final_labels, C_final, true_centers,
                               dataset_name, algo_title, k_clusters, n_features, file_paths, output_dir)

    return saved_file, output_dir

def visualize_clustering_results(X, y_true, final_labels, C_final, true_centers,
                               dataset_name, algo_title, k_clusters, n_features, file_paths=None, output_dir=None):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘å¯¾å¿œï¼‰"""

    # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª1éšå±¤ï¼‰
    if output_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = f"clustering_results_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)

    # è¤‡æ•°ã®æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’è©¦è¡Œ
    reduction_results = {}

    if n_features > 2:
        print(f"\nğŸ“Š æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®æ¯”è¼ƒå®Ÿè¡Œ:")

        # 1. PCA
        print("   ğŸ”„ PCAå®Ÿè¡Œä¸­...")
        pca = PCA(n_components=2, random_state=42)
        X_pca = pca.fit_transform(X)
        C_pca = pca.transform(C_final)
        explained_var_ratio = pca.explained_variance_ratio_
        total_explained_var = np.sum(explained_var_ratio)

        reduction_results['PCA'] = {
            'X_2d': X_pca,
            'C_2d': C_pca,
            'title_suffix': f" (PCA 2D: {total_explained_var*100:.1f}% variance)",
            'info': f"PC1: {explained_var_ratio[0]*100:.1f}%, PC2: {explained_var_ratio[1]*100:.1f}%"
        }

        # 2. t-SNE
        if TSNE_AVAILABLE:
            print("   ğŸ”„ t-SNEå®Ÿè¡Œä¸­...")
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X)-1), max_iter=1000)
            X_tsne = tsne.fit_transform(X)
            # t-SNEã¯å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã¯å¤‰æ›ã§ããªã„ãŸã‚ã€ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¯åˆ¥é€”è¨ˆç®—
            C_tsne = np.array([np.mean(X_tsne[final_labels == i], axis=0) for i in range(len(C_final))])

            reduction_results['t-SNE'] = {
                'X_2d': X_tsne,
                'C_2d': C_tsne,
                'title_suffix': f" (t-SNE 2D)",
                'info': f"perplexity: {min(30, len(X)-1)}, max_iter: 1000"
            }
        else:
            print("   âš ï¸ t-SNEã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

        # 3. UMAP
        if UMAP_AVAILABLE:
            print("   ğŸ”„ UMAPå®Ÿè¡Œä¸­...")
            umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(X)-1))
            X_umap = umap_reducer.fit_transform(X)
            # UMAPã‚‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¯åˆ¥é€”è¨ˆç®—
            C_umap = np.array([np.mean(X_umap[final_labels == i], axis=0) for i in range(len(C_final))])

            reduction_results['UMAP'] = {
                'X_2d': X_umap,
                'C_2d': C_umap,
                'title_suffix': f" (UMAP 2D)",
                'info': f"n_neighbors: {min(15, len(X)-1)}"
            }
        else:
            print("   âš ï¸ UMAPã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

        print(f"   âœ… åˆ©ç”¨å¯èƒ½ãªæ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•: {list(reduction_results.keys())}")
    else:
        # 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
        reduction_results['Original'] = {
            'X_2d': X,
            'C_2d': C_final,
            'title_suffix': "",
            'info': "Original 2D data"
        }

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®è‰²åˆ†ã‘æƒ…å ±ã‚’å–å¾—
    pattern_groups = None
    pattern_colors = None
    pattern_labels = None

    if file_paths is not None and dataset_name == 'real_code_features':
        # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
        target_directory = "../atcoder/submissions_typical90_d_100"

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
        pattern_groups = analyze_file_groups(file_paths, target_directory)

        # è‰²ã®è¨­å®š
        color_palette = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'olive', 'cyan']
        pattern_colors = {}
        color_idx = 0

        for group_name in pattern_groups.keys():
            if group_name == 'other':
                pattern_colors[group_name] = 'gray'
            else:
                pattern_colors[group_name] = color_palette[color_idx % len(color_palette)]
                color_idx += 1

        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
        file_to_group = {}
        for group_name, group_files in pattern_groups.items():
            for file_info in group_files:
                file_to_group[file_info['file_path']] = group_name

        pattern_labels = [file_to_group.get(fp, 'other') for fp in file_paths]

    # å„æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã”ã¨ã«å¯è¦–åŒ–ã‚’å®Ÿè¡Œ
    for method_name, result in reduction_results.items():
        X_2d = result['X_2d']
        C_final_2d = result['C_2d']
        title_suffix = result['title_suffix']
        method_info = result['info']

        print(f"\nğŸ“ˆ {method_name}å¯è¦–åŒ–å®Ÿè¡Œä¸­... ({method_info})")

        # å›³ã‚’ä½œæˆï¼ˆæƒ…å ±è¡¨ç¤ºã‚¹ãƒšãƒ¼ã‚¹ã‚‚ç¢ºä¿ï¼‰
        plt.figure(figsize=(18, 8))

        # å¯†é›†åº¦ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šã‚’èª¿æ•´
        n_points = len(X_2d)
        if n_points > 100:
            point_size = max(30, 100 - n_points // 10)  # ç‚¹æ•°ãŒå¤šã„ã»ã©å°ã•ã
            alpha_val = max(0.6, 1.0 - n_points / 500)  # ç‚¹æ•°ãŒå¤šã„ã»ã©é€æ˜ã«
        else:
            point_size = 60
            alpha_val = 0.8

        # å·¦å´: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è‰²åˆ†ã‘ï¼ˆå…¨ä½“è¡¨ç¤ºï¼‰
        plt.subplot(1, 2, 1)
        if pattern_groups is not None:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
            for group_name in pattern_groups.keys():
                group_indices = [i for i, label in enumerate(pattern_labels) if label == group_name]
                if group_indices:
                    group_points = X_2d[group_indices]
                    plt.scatter(group_points[:, 0], group_points[:, 1],
                               c=pattern_colors[group_name],
                               label=f'{group_name} ({len(group_indices)})',
                               alpha=alpha_val, s=point_size, edgecolors='black', linewidth=0.5)

            plt.title(f"Pattern-based Grouping ({method_name})\n{dataset_name.capitalize()} Dataset{title_suffix}", fontsize=12)
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)

        else:
            plt.scatter(X_2d[:, 0], X_2d[:, 1], c='gray', alpha=alpha_val, s=point_size)
            plt.title(f"Original Data ({method_name})\n{dataset_name.capitalize()} Dataset{title_suffix}")

        # è»¸ã®ç¯„å›²ã‚’é©åˆ‡ã«è¨­å®šï¼ˆè² ã®å€¤ã‚‚è¦‹ã‚„ã™ãï¼‰
        x_margin = (np.max(X_2d[:, 0]) - np.min(X_2d[:, 0])) * 0.05
        y_margin = (np.max(X_2d[:, 1]) - np.min(X_2d[:, 1])) * 0.05
        plt.xlim(np.min(X_2d[:, 0]) - x_margin, np.max(X_2d[:, 0]) + x_margin)
        plt.ylim(np.min(X_2d[:, 1]) - y_margin, np.max(X_2d[:, 1]) + y_margin)

        plt.xlabel(f"{method_name} Component 1" if n_features > 2 else "Feature 1", fontsize=11)
        plt.ylabel(f"{method_name} Component 2" if n_features > 2 else "Feature 2", fontsize=11)
        plt.grid(True, alpha=0.4)

        # å³å´: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆå…¨ä½“è¡¨ç¤ºï¼‰
        plt.subplot(1, 2, 2)
        scatter2 = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=final_labels, cmap='tab10',
                              alpha=alpha_val, s=point_size, edgecolors='black', linewidth=0.5)
        plt.title(f"{algo_title} Results ({method_name})\n{dataset_name.capitalize()} Dataset{title_suffix}", fontsize=12)

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºï¼ˆã‚«ãƒ©ãƒ¼ãƒãƒ¼ã®ä»£ã‚ã‚Šï¼‰
        unique_clusters = np.unique(final_labels)

        # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è‰²ã‚’å–å¾—ï¼ˆtab10ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ä½¿ç”¨ï¼‰
        tab10_colors = cm.get_cmap('tab10')

        # è‰²ã®å‡¡ä¾‹ã‚’å€‹åˆ¥ã«ä½œæˆï¼ˆå³ä¸Šã«é…ç½®ï¼‰
        legend_elements = []
        for cluster_id in unique_clusters:
            cluster_count = np.sum(final_labels == cluster_id)
            color_rgb = tab10_colors(cluster_id / 10.0)
            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w',
                                            markerfacecolor=color_rgb, markersize=8,
                                            label=f'Cluster {cluster_id} ({cluster_count} files)'))

        # æœ€çµ‚ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        scatter_centroids = plt.scatter(C_final_2d[:, 0], C_final_2d[:, 1],
                   c='red', s=250, marker='X', edgecolor='black', linewidth=2,
                   alpha=1.0)

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã®å‡¡ä¾‹ã‚’è¿½åŠ 
        legend_elements.append(plt.Line2D([0], [0], marker='X', color='w',
                                        markerfacecolor='red', markersize=12, markeredgecolor='black',
                                        label='Final Centroids'))

        # çµ±åˆã•ã‚ŒãŸå‡¡ä¾‹ã‚’è¡¨ç¤º
        plt.legend(handles=legend_elements, loc='upper right', fontsize=9,
                  bbox_to_anchor=(0.98, 0.98), framealpha=0.9)

        # è»¸ã®ç¯„å›²ã‚’é©åˆ‡ã«è¨­å®šï¼ˆè² ã®å€¤ã‚‚è¦‹ã‚„ã™ãï¼‰
        plt.xlim(np.min(X_2d[:, 0]) - x_margin, np.max(X_2d[:, 0]) + x_margin)
        plt.ylim(np.min(X_2d[:, 1]) - y_margin, np.max(X_2d[:, 1]) + y_margin)

        plt.xlabel(f"{method_name} Component 1" if n_features > 2 else "Feature 1", fontsize=11)
        plt.ylabel(f"{method_name} Component 2" if n_features > 2 else "Feature 2", fontsize=11)
        plt.grid(True, alpha=0.4)

        plt.tight_layout()

        # ç”»åƒã¨ã—ã¦ä¿å­˜ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æ‰‹æ³•åä»˜ãï¼‰
        method_filename = method_name.lower().replace('-', '_')
        # output_dirã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º
        timestamp = output_dir.split('_')[-1] if 'clustering_results_' in output_dir else datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = os.path.join(output_dir, f"clustering_result_{dataset_name}_{algo_title.lower().replace(' ', '_').replace('-', '_')}_{method_filename}_{timestamp}.png")
        plt.savefig(filename, dpi=200, bbox_inches='tight')
        print(f"ğŸ“¸ {method_name}å¯è¦–åŒ–çµæœã‚’ '{filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

        plt.show()

        # æ‰‹æ³•åˆ¥ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        print(f"\nğŸ“Š {method_name}çµ±è¨ˆæƒ…å ±:")
        print(f"  ãƒ‡ãƒ¼ã‚¿ç¯„å›² Dim1: [{np.min(X_2d[:, 0]):.2f}, {np.max(X_2d[:, 0]):.2f}]")
        print(f"  ãƒ‡ãƒ¼ã‚¿ç¯„å›² Dim2: [{np.min(X_2d[:, 1]):.2f}, {np.max(X_2d[:, 1]):.2f}]")
        print(f"  æ¨™æº–åå·® Dim1: {np.std(X_2d[:, 0]):.2f}")
        print(f"  æ¨™æº–åå·® Dim2: {np.std(X_2d[:, 1]):.2f}")
        print(f"  æ‰‹æ³•æƒ…å ±: {method_info}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœã®å‡ºåŠ›ï¼ˆãƒ—ãƒ­ãƒƒãƒˆã®å¤–ã§è¡¨ç¤ºï¼‰
    if pattern_groups is not None:
        print(f"\nğŸ¨ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ:")
        for group_name, group_files in pattern_groups.items():
            group_count = len([f for f in group_files if f['file_path'] in file_paths])
            print(f"  {group_name}: {group_count} ãƒ•ã‚¡ã‚¤ãƒ« (è‰²: {pattern_colors[group_name]})")

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœã®å‡ºåŠ›ï¼ˆãƒ—ãƒ­ãƒƒãƒˆã®å¤–ã§è¡¨ç¤ºï¼‰
    unique_clusters = np.unique(final_labels)
    print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœï¼ˆè©³ç´°ï¼‰:")
    print("=" * 80)

    if file_paths is not None:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆJSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
        file_names_from_paths = [path.split('/')[-1] for path in file_paths]

        for cluster_id in unique_clusters:
            cluster_mask = final_labels == cluster_id
            cluster_files = [file_names_from_paths[i] for i in range(len(file_names_from_paths)) if cluster_mask[i]]
            cluster_paths = [file_paths[i] for i in range(len(file_paths)) if cluster_mask[i]]

            print(f"\nğŸ·ï¸ Cluster {cluster_id} ({len(cluster_files)} ãƒ•ã‚¡ã‚¤ãƒ«):")

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®åˆ†å¸ƒã‚’åˆ†æ
            pattern_distribution = {}
            for path in cluster_paths:
                if 'pattern1' in path:
                    pattern_distribution['pattern1'] = pattern_distribution.get('pattern1', 0) + 1
                elif 'pattern2' in path:
                    pattern_distribution['pattern2'] = pattern_distribution.get('pattern2', 0) + 1
                elif 'pattern3' in path:
                    pattern_distribution['pattern3'] = pattern_distribution.get('pattern3', 0) + 1
                elif 'pattern4' in path:
                    pattern_distribution['pattern4'] = pattern_distribution.get('pattern4', 0) + 1
                else:
                    pattern_distribution['other'] = pattern_distribution.get('other', 0) + 1

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒã‚’è¡¨ç¤º
            if pattern_distribution:
                pattern_info = ", ".join([f"{pattern}: {count}" for pattern, count in pattern_distribution.items()])
                print(f"   ğŸ“‚ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ: {pattern_info}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            print(f"   ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
            sorted_files = sorted(zip(cluster_files, cluster_paths))
            for i, (filename, filepath) in enumerate(sorted_files, 1):
                # ãƒ‘ã‚¹ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’æŠ½å‡º
                if 'pattern' in filepath:
                    pattern_info = filepath.split('/')[-2] if '/' in filepath else 'unknown'
                else:
                    pattern_info = 'root'
                print(f"      {i:2d}. {filename:<25} ({pattern_info})")

                if i >= 15 and len(sorted_files) > 15:  # 15å€‹ã¾ã§è¡¨ç¤º
                    remaining = len(sorted_files) - 15
                    print(f"         ... ãŠã‚ˆã³ {remaining} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
                    break
    else:
        for cluster_id in unique_clusters:
            cluster_count = np.sum(final_labels == cluster_id)
            print(f"  Cluster {cluster_id}: {cluster_count} ãƒ•ã‚¡ã‚¤ãƒ«")

    print("=" * 80)

if __name__ == '__main__':

    # --- å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ---
    if FEATURE_EXTRACTION_AVAILABLE:
        print("\n=== Real Code Features Dataset: å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ç‰¹å¾´é‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ===")

        saved_files = []

        try:
            # 1. ä¸€èˆ¬çš„ãªK-meansã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè¡Œ
            print("\nğŸ”„ ä¸€èˆ¬çš„ãªK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
            general_result_file, general_output_dir = main(algorithm_type='general', dataset_name='real_code_features')
            saved_files.append(('general', general_result_file, general_output_dir))

        except Exception as e:
            print(f"âŒ ä¸€èˆ¬çš„ãªK-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")
            print("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        try:
            # 2. æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
            print("\nğŸ¯ æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
            correctness_result_file, correctness_output_dir = main(algorithm_type='correctness_guided', dataset_name='real_code_features')
            saved_files.append(('correctness_guided', correctness_result_file, correctness_output_dir))

        except Exception as e:
            print(f"âŒ æ­£è§£åˆ¤å®šé–¢æ•°ã‚’åˆ©ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")
            print("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        print(f"\n{'='*80}")
        print("ğŸ‰ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œå®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")

        if saved_files:
            for algorithm_type, result_file, output_dir in saved_files:
                if result_file:
                    print(f"âœ… {algorithm_type.upper()}:")
                    print(f"   ğŸ“ çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
                    print(f"   ğŸ“„ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(result_file)}")
                else:
                    print(f"âŒ {algorithm_type.upper()}: çµæœä¿å­˜ã«å¤±æ•—")
        else:
            print("âŒ ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

        print(f"{'='*80}")

    else:
        print("âš ï¸ ç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
