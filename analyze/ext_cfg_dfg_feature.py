# CFGç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ext-cfg-feature.pyã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç°¡æ½”ã«æŠ½å‡º
# ext_feature_data_flow.pyã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã‚‚æŠ½å‡º

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import matplotlib.colors as mcolors
import json
import pickle
from datetime import datetime

# ext-cfg-feature.pyã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    # control-flowãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
    control_flow_path = os.path.join(os.path.dirname(__file__), 'control-flow')
    sys.path.append(control_flow_path)

    from ext_cfg_feature import analyze_accurate_cfg
except ImportError as e:
    print(f"âŒ CFGç‰¹å¾´é‡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ext-cfg-feature.pyãŒ control-flow/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

# ext_feature_data_flow.pyã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    # data-flowãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
    data_flow_path = os.path.join(os.path.dirname(__file__), 'data-flow')
    sys.path.append(data_flow_path)

    from ext_feature_data_flow import extract_dataflow_features_as_list, get_dataflow_feature_vector
except ImportError as e:
    print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ext_feature_data_flow.pyãŒ data-flow/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

def extract_dataflow_features_vector(source_file):
    """
    ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º

    Args:
        source_file (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        list: [total_reads, total_writes, max_reads, max_writes, var_count]
    """
    try:
        # ext_feature_data_flow.pyã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—
        dataflow_vector = get_dataflow_feature_vector(source_file)
        return dataflow_vector

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return [0, 0, 0, 0, 0]

def get_dataflow_feature_names():
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã®åå‰ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    return [
        'total_reads',        # ç·èª­ã¿è¾¼ã¿æ•°
        'total_writes',       # ç·æ›¸ãè¾¼ã¿æ•°
        'max_reads',          # èª­ã¿è¾¼ã¿æ•°æœ€å¤§å€¤
        'max_writes',         # æ›¸ãè¾¼ã¿æ•°æœ€å¤§å€¤
        'var_count'           # å¤‰æ•°ç¨®é¡æ•°
    ]

def extract_integrated_features_vector(source_file):
    """
    CFGç‰¹å¾´é‡ã¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã‚’çµ±åˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º

    Args:
        source_file (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        list: çµ±åˆã•ã‚ŒãŸç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ« [CFG(6æ¬¡å…ƒ) + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼(5æ¬¡å…ƒ)]
    """
    try:
        # CFGç‰¹å¾´é‡ã‚’å–å¾—
        cfg_vector = extract_cfg_features_vector(source_file)

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã‚’å–å¾—
        dataflow_vector = extract_dataflow_features_vector(source_file)

        # çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
        integrated_vector = cfg_vector + dataflow_vector

        return integrated_vector

    except Exception as e:
        print(f"âŒ çµ±åˆç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

def extract_cfg_features_vector(source_file):
    """
    ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰CFGç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º

    Args:
        source_file (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        list: [connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity]
    """
    try:
        # print(f"ğŸ”„ CFGç‰¹å¾´é‡æŠ½å‡ºä¸­: {source_file}")

        # ext-cfg-feature.pyã®analyze_accurate_cfgé–¢æ•°ã‚’å‘¼ã³å‡ºã—
        all_features = analyze_accurate_cfg(source_file)

        if not all_features:
            print("âš ï¸  CFGç‰¹å¾´é‡ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return [0, 0, 0, 0, 0, 0]

        # é–¢æ•°ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã¯é™¤å¤–ï¼‰
        function_features = {k: v for k, v in all_features.items()
                           if not (k.startswith('<module>') or k.startswith('&lt;module&gt;'))}

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆæ§‹é€ çš„ç‰¹å¾´ã®ã¿ï¼‰
        module_features = {k: v for k, v in all_features.items()
                         if k.startswith('<module>') or k.startswith('&lt;module&gt;')}

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        # 1. connected_components: è«–ç†ç©ï¼ˆ1ã¤ã§ã‚‚0ãŒã‚ã‚Œã°0ã€å…¨ã¦1ä»¥ä¸Šãªã‚‰1ï¼‰
        all_connected_components = [features.get('connected_components', 0) for features in all_features.values()]
        total_connected = 1 if all(cc > 0 for cc in all_connected_components) else 0

        # 2. ãƒ«ãƒ¼ãƒ—ã¨æ¡ä»¶æ–‡: é–¢æ•°å˜ä½ã§æ—¢ã«æ­£ç¢ºã«è¨ˆç®—æ¸ˆã¿ï¼ˆå†å¸°å«ã‚€ï¼‰
        total_loops = sum(features.get('loop_statements', 0) for features in function_features.values())
        total_conditions = sum(features.get('conditional_statements', 0) for features in function_features.values())

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®åˆ†ã‚‚è¿½åŠ 
        total_loops += sum(features.get('loop_statements', 0) for features in module_features.values())
        total_conditions += sum(features.get('conditional_statements', 0) for features in module_features.values())

        # 3. æ§‹é€ çš„ç‰¹å¾´: å…¨ä½“ã‹ã‚‰è¨ˆç®—ï¼ˆé–¢æ•°+ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
        total_cycles = sum(features.get('cycles', 0) for features in all_features.values())
        total_paths = sum(features.get('paths', 0) for features in all_features.values())
        total_complexity = sum(features.get('cyclomatic_complexity', 0) for features in all_features.values())

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
        clustering_vector = [
            total_connected,     # 1. é€£çµæˆåˆ†æ•°
            total_loops,         # 2. ãƒ«ãƒ¼ãƒ—æ–‡æ•°ï¼ˆå†å¸°å«ã‚€ï¼‰
            total_conditions,    # 3. æ¡ä»¶æ–‡æ•°
            total_cycles,        # 4. ã‚µã‚¤ã‚¯ãƒ«æ•°
            total_paths,         # 5. ãƒ‘ã‚¹æ•°ï¼ˆãƒ«ãƒ¼ãƒ—è€ƒæ…®ç‰ˆï¼‰
            total_complexity     # 6. ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦
        ]

        # print(f"âœ… CFGç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {clustering_vector}")
        return clustering_vector

    except Exception as e:
        print(f"âŒ CFGç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return [0, 0, 0, 0, 0, 0]

def get_cfg_feature_names():
    """CFGç‰¹å¾´é‡ã®åå‰ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    return [
        'connected_components',    # é€£çµæˆåˆ†æ•°
        'loop_statements',         # ãƒ«ãƒ¼ãƒ—æ–‡æ•°ï¼ˆå†å¸°å«ã‚€ï¼‰
        'conditional_statements',  # æ¡ä»¶æ–‡æ•°
        'cycles',                  # ã‚µã‚¤ã‚¯ãƒ«æ•°
        'paths',                   # ãƒ‘ã‚¹æ•°ï¼ˆãƒ«ãƒ¼ãƒ—è€ƒæ…®ç‰ˆï¼‰
        'cyclomatic_complexity'    # ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦
    ]

def batch_extract_integrated_features(file_list):
    """
    è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆç‰¹å¾´é‡ã‚’ä¸€æ‹¬æŠ½å‡º

    Args:
        file_list (list): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

    Returns:
        list: å„ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ãƒªã‚¹ãƒˆ
    """
    results = []

    print(f"ğŸ“‚ çµ±åˆç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹: {len(file_list)}ãƒ•ã‚¡ã‚¤ãƒ«")

    for i, source_file in enumerate(file_list, 1):
        try:
            result = extract_integrated_features_vector(source_file)
            results.append({
                'source_file': source_file,
                'integrated_vector': result
            })

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
            results.append({
                'source_file': source_file,
                'integrated_vector': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                'error': str(e)
            })

    return results

def batch_extract_cfg_features(file_list):
    """
    è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®CFGç‰¹å¾´é‡ã‚’ä¸€æ‹¬æŠ½å‡º

    Args:
        file_list (list): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

    Returns:
        list: å„ãƒ•ã‚¡ã‚¤ãƒ«ã®CFGç‰¹å¾´é‡çµæœãƒªã‚¹ãƒˆ
    """
    results = []

    print(f"ğŸ“‚ CFGç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹: {len(file_list)}ãƒ•ã‚¡ã‚¤ãƒ«")

    for i, source_file in enumerate(file_list, 1):
        try:
            result = extract_cfg_features_vector(source_file)
            results.append(result)

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
            error_result = {
                'source_file': source_file,
                'clustering_vector': [0, 0, 0, 0, 0, 0],
                'feature_names': get_cfg_feature_names(),
                'individual_features': {},
                'summary': {'total_functions': 0, 'total_modules': 0},
                'error': str(e)
            }
            results.append(error_result)

    return results

def find_files_in_directory(directory, file_extensions=['.py', '.c', '.cpp', '.java']):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹

    Args:
        directory (str): æ¤œç´¢å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        file_extensions (list): å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒªã‚¹ãƒˆ

    Returns:
        list: ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    found_files = []

    def explore_directory(current_dir):
        try:
            # lsã‚³ãƒãƒ³ãƒ‰ç›¸å½“: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’å–å¾—
            contents = os.listdir(current_dir)

            # .ã‹ã‚‰å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é™¤å¤–
            filtered_contents = [item for item in contents if not item.startswith('.')]

            for item in filtered_contents:
                item_path = os.path.join(current_dir, item)

                if os.path.isfile(item_path):
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ: æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
                    _, ext = os.path.splitext(item)
                    if ext.lower() in file_extensions:
                        found_files.append(item_path)

                elif os.path.isdir(item_path):
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ: å†å¸°çš„ã«æ¢ç´¢
                    explore_directory(item_path)

        except (FileNotFoundError, PermissionError) as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {current_dir} - {e}")

    explore_directory(directory)

    return found_files

def analyze_file_groups(file_list, base_directory):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã—ã¦åˆ†æï¼ˆå‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜å¯¾å¿œï¼‰

    Args:
        file_list (list): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

    Returns:
        dict: ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœ
    """
    import re

    groups = {}

    for file_path in file_list:
        relative_path = os.path.relpath(file_path, base_directory)
        path_parts = relative_path.split(os.sep)

        # å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºï¼ˆkmeans_final_clean.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        group_name = extract_pattern_from_file_path(file_path)

        if group_name not in groups:
            groups[group_name] = []

        groups[group_name].append({
            'file_path': file_path,
            'relative_path': relative_path,
            'filename': os.path.basename(file_path),
            'pattern': group_name
        })

    return groups

def extract_pattern_from_file_path(filepath):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’å‹•çš„ã«æŠ½å‡º

    Args:
        filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        str: ãƒ‘ã‚¿ãƒ¼ãƒ³å (ä¾‹: "typical90_aa", "typical90_d", "AC", "TLE")
    """
    import re

    # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã«å¤‰æ›ï¼‰
    normalized_path = filepath.replace('\\', '/')

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹æ­£è¦è¡¨ç¾ã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    pattern_regexes = [
        # submissions_typical90_xx ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæœ€å„ªå…ˆï¼‰
        (r'submissions_typical90_([a-z]+)', lambda m: f"typical90_{m.group(1)}"),
        # pattern + æ•°å­—
        (r'pattern(\d+)', lambda m: f"pattern{m.group(1)}"),
        # AC, TLE ãªã©ã®çµæœãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ˜ç¢ºãªã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢åŒºåˆ‡ã‚Šï¼‰
        (r'_([A-Z]{2,3})(?:_|$|/)', lambda m: m.group(1)),
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãŒçµæœã‚’è¡¨ã™å ´åˆ
        (r'/([A-Z]{2,3})/', lambda m: m.group(1)),
        # ãã®ä»–ã®submissions_ãƒ‘ã‚¿ãƒ¼ãƒ³
        (r'submissions_([^/]+?)(?:_\d+)?/', lambda m: m.group(1) if not m.group(1).startswith('submission') else None),
    ]

    for pattern_regex, extract_func in pattern_regexes:
        match = re.search(pattern_regex, normalized_path)
        if match:
            result = extract_func(match)
            if result:
                # ä¸€èˆ¬çš„ã§ãªã„å½¢å¼ã‚„çŸ­ã™ãã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
                if len(result) >= 2 and not result.isdigit():
                    return result

    # ã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚‚ä¸€è‡´ã—ãªã„å ´åˆ
    # ãŸã ã—ã€æ˜ã‚‰ã‹ã«ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã¯å†è©¦è¡Œ
    filename = os.path.basename(filepath)

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã™ã‚‹æœ€å¾Œã®è©¦è¡Œ
    filename_patterns = [
        (r'^([a-z]+\d*)_', lambda m: m.group(1)),  # prefix_xxxå½¢å¼
        (r'_([a-z]+\d*)\.', lambda m: m.group(1)), # xxx_suffix.extå½¢å¼
    ]

    for pattern_regex, extract_func in filename_patterns:
        match = re.search(pattern_regex, filename.lower())
        if match:
            result = extract_func(match)
            if result and len(result) >= 2:
                return result

    return "other"

def save_feature_vectors(batch_results, groups=None, base_directory=None, output_file=None, format='json'):
    """
    ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚‚å«ã‚€ï¼‰

    Args:
        batch_results (list): ç‰¹å¾´é‡æŠ½å‡ºçµæœ
        groups (dict): ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ç”¨ï¼‰
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ç”¨ï¼‰
        output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        format (str): ä¿å­˜å½¢å¼ ('json' ã¾ãŸã¯ 'pickle')
    """
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if format == 'json':
            output_file = f"feature_vectors_{timestamp}.json"
        else:
            output_file = f"feature_vectors_{timestamp}.pkl"

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆå·®åˆ†æ¤œå‡ºç”¨ï¼‰
        file_metadata = {}
        for result in batch_results:
            if 'source_file' in result:
                file_path = result['source_file']
                try:
                    if os.path.exists(file_path):
                        mtime = os.path.getmtime(file_path)
                        size = os.path.getsize(file_path)
                        file_metadata[file_path] = {
                            'mtime': mtime,
                            'size': size,
                            'timestamp': datetime.fromtimestamp(mtime).isoformat()
                        }
                except Exception as e:
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        save_data = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(batch_results),
            'successful_extractions': len([r for r in batch_results if 'error' not in r]),
            'feature_names': {
                'cfg_features': get_cfg_feature_names(),
                'dataflow_features': get_dataflow_feature_names()
            },
            'file_metadata': file_metadata,  # å·®åˆ†æ¤œå‡ºç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            'data': batch_results
        }

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚‚è¨ˆç®—ãƒ»ä¿å­˜
        if groups is not None and base_directory is not None:
            centroids_data = calculate_pattern_centroids(batch_results, groups, base_directory)

            if centroids_data and centroids_data['centroids']:
                save_data['pattern_centroids'] = centroids_data
                print(f"âœ… ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¿½åŠ : {len(centroids_data['centroids'])}å€‹")
            else:
                save_data['pattern_centroids'] = None
        else:
            save_data['pattern_centroids'] = None

        if format == 'json':
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
        elif format == 'pickle':
            with open(output_file, 'wb') as f:
                pickle.dump(save_data, f)
        else:
            raise ValueError("format ã¯ 'json' ã¾ãŸã¯ 'pickle' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

        print(f"ğŸ’¾ ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜: '{output_file}' ({format.upper()})")
        print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«: {save_data['total_files']}, æˆåŠŸ: {save_data['successful_extractions']}")
        if save_data['pattern_centroids']:
            print(f"   ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰: {len(save_data['pattern_centroids']['centroids'])}å€‹")

        return output_file

    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_feature_vectors(input_file):
    """
    ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿

    Args:
        input_file (str): å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        dict: èª­ã¿è¾¼ã¾ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    """
    try:
        if input_file.endswith('.json'):
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        elif input_file.endswith('.pkl'):
            with open(input_file, 'rb') as f:
                data = pickle.load(f)
        else:
            raise ValueError("ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆ.json ã¾ãŸã¯ .pkl ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰")

        print(f"ğŸ“‚ ç‰¹å¾´é‡èª­ã¿è¾¼ã¿: '{input_file}'")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {data['total_files']}, æˆåŠŸ: {data['successful_extractions']}")

        return data

    except Exception as e:
        print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def calculate_pattern_centroids(batch_results, groups, base_directory):
    """
    å„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é‡å¿ƒï¼ˆçœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼‰ã‚’è¨ˆç®—

    Args:
        batch_results (list): ç‰¹å¾´é‡æŠ½å‡ºçµæœ
        groups (dict): ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœ
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

    Returns:
        dict: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±
    """
    # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
    successful_results = [r for r in batch_results if 'error' not in r]

    if len(successful_results) == 0:
        return {}

    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    feature_vectors = np.array([r['integrated_vector'] for r in successful_results])
    file_paths = [r['source_file'] for r in successful_results]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    file_to_group = {}
    for group_name, group_files in groups.items():
        for file_info in group_files:
            file_to_group[file_info['file_path']] = group_name

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹ï¼ˆå¤–ã‚Œå€¤otherã¯é™¤å¤–ï¼‰
    pattern_groups = {k: v for k, v in groups.items() if k.startswith('pattern')}

    # å¤–ã‚Œå€¤('other')ã‚°ãƒ«ãƒ¼ãƒ—ã¯çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å«ã‚ãªã„
    if 'other' in groups and len(groups['other']) > 0:
        print(f"â„¹ï¸  å¤–ã‚Œå€¤é™¤å¤–: {len(groups['other'])}ãƒ•ã‚¡ã‚¤ãƒ« (æ—¢å­˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åˆ†æ•£é…ç½®)")

    centroids_data = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'base_directory': base_directory,
            'total_patterns': len(pattern_groups),
            'excludes_other_group': True,  # å¤–ã‚Œå€¤(other)ã¯é™¤å¤–
            'meaningful_patterns_only': True,  # æ„å‘³ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
            'feature_dimension': len(feature_vectors[0]) if len(feature_vectors) > 0 else 0,
            'feature_names': {
                'cfg_features': get_cfg_feature_names(),
                'dataflow_features': get_dataflow_feature_names()
            }
        },
        'centroids': {}
    }

    print(f"ğŸ¯ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—: {len(pattern_groups)}å€‹ã®æ„å‘³ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³")

    for pattern_name, pattern_files in pattern_groups.items():
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å±ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        pattern_indices = []
        pattern_file_paths = []

        for i, file_path in enumerate(file_paths):
            if file_to_group.get(file_path) == pattern_name:
                pattern_indices.append(i)
                pattern_file_paths.append(file_path)

        if not pattern_indices:
            continue

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
        pattern_vectors = feature_vectors[pattern_indices]

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼ˆé‡å¿ƒï¼‰ã‚’è¨ˆç®—
        centroid = np.mean(pattern_vectors, axis=0).tolist()

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’ä¿å­˜
        centroids_data['centroids'][pattern_name] = {
            'centroid_vector': centroid,
            'sample_count': len(pattern_indices),
            'file_paths': pattern_file_paths
        }

        print(f"   {pattern_name}: {len(pattern_indices)}ãƒ•ã‚¡ã‚¤ãƒ«")

    return centroids_data

def save_pattern_centroids(centroids_data, output_file=None):
    """
    ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        centroids_data (dict): ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿
        output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰

    Returns:
        str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
    """
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_dir = centroids_data['metadata'].get('base_directory', 'unknown')
        base_name = os.path.basename(base_dir) if base_dir != 'unknown' else 'patterns'
        output_file = f"pattern_centroids_{base_name}_{timestamp}.json"

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(centroids_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä¿å­˜: '{output_file}'")
        print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {centroids_data['metadata']['total_patterns']}")

        return output_file

    except Exception as e:
        print(f"âŒ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_pattern_centroids(input_file):
    """
    ä¿å­˜ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿

    Args:
        input_file (str): å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        dict: èª­ã¿è¾¼ã¾ã‚ŒãŸã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿
    """
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"ğŸ“‚ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰èª­ã¿è¾¼ã¿: '{input_file}'")
        print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {data['metadata']['total_patterns']}")

        return data

    except Exception as e:
        print(f"âŒ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def check_cache_validity(target_directory, cache_file):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        target_directory (str): å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        cache_file (str): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        bool: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹
    """
    if not os.path.exists(cache_file):
        return False

    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’å–å¾—
        cache_mtime = os.path.getmtime(cache_file)

        # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€æ–°æ›´æ–°æ™‚åˆ»ã‚’ãƒã‚§ãƒƒã‚¯
        latest_file_mtime = 0
        for root, dirs, files in os.walk(target_directory):
            for file in files:
                if file.endswith(('.py', '.c', '.cpp', '.java')):
                    file_path = os.path.join(root, file)
                    file_mtime = os.path.getmtime(file_path)
                    latest_file_mtime = max(latest_file_mtime, file_mtime)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚ˆã‚Šæ–°ã—ã‘ã‚Œã°æœ‰åŠ¹
        return cache_mtime > latest_file_mtime

    except Exception as e:
        print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def detect_file_changes(target_directory, cache_file):
    """
    å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®å·®åˆ†ã‚’æ¤œå‡º

    Args:
        target_directory (str): å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        cache_file (str): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        dict: {
            'new_files': [],      # æ–°è¦è¿½åŠ ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            'modified_files': [], # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            'deleted_files': [],  # å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            'unchanged_files': [] # å¤‰æ›´ãªã—ãƒ•ã‚¡ã‚¤ãƒ«
        }
    """
    print("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å·®åˆ†æ¤œå‡ºä¸­...")

    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    current_files = find_files_in_directory(target_directory)
    current_file_info = {}

    for file_path in current_files:
        try:
            mtime = os.path.getmtime(file_path)
            size = os.path.getsize(file_path)
            current_file_info[file_path] = {
                'mtime': mtime,
                'size': size
            }
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            continue

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    cached_file_info = {}
    if os.path.exists(cache_file):
        try:
            cached_data = load_feature_vectors(cache_file)
            if cached_data and 'file_metadata' in cached_data:
                cached_file_info = cached_data['file_metadata']
            elif cached_data and 'data' in cached_data:
                # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æƒ…å ±ã‚’å†æ§‹ç¯‰
                for item in cached_data['data']:
                    if 'source_file' in item:
                        file_path = item['source_file']
                        if os.path.exists(file_path):
                            try:
                                mtime = os.path.getmtime(file_path)
                                size = os.path.getsize(file_path)
                                cached_file_info[file_path] = {
                                    'mtime': mtime,
                                    'size': size
                                }
                            except:
                                pass
        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # å·®åˆ†ã‚’è¨ˆç®—
    current_files_set = set(current_files)
    cached_files_set = set(cached_file_info.keys())

    # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«
    new_files = list(current_files_set - cached_files_set)

    # å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
    deleted_files = list(cached_files_set - current_files_set)

    # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¨å¤‰æ›´ãªã—ãƒ•ã‚¡ã‚¤ãƒ«
    modified_files = []
    unchanged_files = []

    for file_path in current_files_set & cached_files_set:
        current_info = current_file_info.get(file_path, {})
        cached_info = cached_file_info.get(file_path, {})

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¾ãŸã¯æ›´æ–°æ™‚åˆ»ãŒç•°ãªã‚‹å ´åˆã¯å¤‰æ›´ã‚ã‚Š
        if (current_info.get('mtime', 0) != cached_info.get('mtime', 0) or
            current_info.get('size', 0) != cached_info.get('size', 0)):
            modified_files.append(file_path)
        else:
            unchanged_files.append(file_path)

    changes = {
        'new_files': new_files,
        'modified_files': modified_files,
        'deleted_files': deleted_files,
        'unchanged_files': unchanged_files
    }

    # å·®åˆ†æƒ…å ±ã‚’è¡¨ç¤º
    print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«å·®åˆ†: æ–°è¦{len(new_files)} å¤‰æ›´{len(modified_files)} å‰Šé™¤{len(deleted_files)} å¤‰æ›´ãªã—{len(unchanged_files)}")

    return changes

def update_cache_incrementally(target_directory, cache_file, file_changes):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«å·®åˆ†ã«åŸºã¥ã„ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¢—åˆ†æ›´æ–°

    Args:
        target_directory (str): å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        cache_file (str): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«
        file_changes (dict): detect_file_changes()ã®æˆ»ã‚Šå€¤

    Returns:
        list: æ›´æ–°å¾Œã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    """
    print("ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¢—åˆ†æ›´æ–°ä¸­...")

    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿
    existing_data = []
    existing_metadata = {}
    if os.path.exists(cache_file):
        try:
            cached_data = load_feature_vectors(cache_file)
            if cached_data and 'data' in cached_data:
                existing_data = cached_data['data']
            if cached_data and 'file_metadata' in cached_data:
                existing_metadata = cached_data['file_metadata']
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # å¤‰æ›´ãªã—ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
    preserved_data = []
    for item in existing_data:
        if 'source_file' in item and item['source_file'] in file_changes['unchanged_files']:
            preserved_data.append(item)

    # æ–°è¦ãƒ»å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    files_to_process = file_changes['new_files'] + file_changes['modified_files']
    new_data = []

    if files_to_process:
        new_data = batch_extract_integrated_features(files_to_process)

    # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    updated_data = preserved_data + new_data

    print(f"ğŸ“¦ ä¿æŒ: {len(preserved_data)}, æ–°è¦å‡¦ç†: {len(new_data)}, ç·è¨ˆ: {len(updated_data)}")

    return updated_data

def visualize_feature_distribution(batch_results, groups, base_directory):
    """
    ç‰¹å¾´é‡ã®åˆ†å¸ƒã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«å¯è¦–åŒ–

    Args:
        batch_results (list): ç‰¹å¾´é‡æŠ½å‡ºçµæœ
        groups (dict): ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœ
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
    """
    # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
    successful_results = [r for r in batch_results if 'error' not in r]

    if len(successful_results) == 0:
        print("âŒ å¯è¦–åŒ–å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    feature_vectors = np.array([r['integrated_vector'] for r in successful_results])
    file_paths = [r['source_file'] for r in successful_results]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    file_to_group = {}
    group_colors = {}
    color_palette = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'olive', 'cyan']

    color_idx = 0
    for group_name, group_files in groups.items():
        if group_name == 'other':
            group_colors[group_name] = 'gray'
        else:
            group_colors[group_name] = color_palette[color_idx % len(color_palette)]
            color_idx += 1

        for file_info in group_files:
            file_to_group[file_info['file_path']] = group_name

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è‰²ã¨ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
    colors = []
    labels = []
    for file_path in file_paths:
        group = file_to_group.get(file_path, 'other')
        colors.append(group_colors[group])
        labels.append(group)

    # PCAã§2æ¬¡å…ƒã«æ¬¡å…ƒå‰Šæ¸›
    print("ğŸ“Š PCAå¯è¦–åŒ–ä¸­...")
    pca = PCA(n_components=2)
    feature_vectors_2d = pca.fit_transform(feature_vectors)

    # å…¨ä½“ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(12, 8))

    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ãƒ—ãƒ­ãƒƒãƒˆ
    for group_name in groups.keys():
        group_indices = [i for i, label in enumerate(labels) if label == group_name]
        if group_indices:
            group_points = feature_vectors_2d[group_indices]
            plt.scatter(group_points[:, 0], group_points[:, 1],
                       c=group_colors[group_name],
                       label=f'{group_name} ({len(group_indices)})',
                       alpha=0.7, s=60)

    plt.title(f'Feature Distribution\n{base_directory}', fontsize=14)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})', fontsize=12)
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"feature_distribution_{timestamp}.png"
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    print(f"ğŸ“¸ å¯è¦–åŒ–ä¿å­˜: {filename}")

    plt.show()

    # çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    print(f"ğŸ“ˆ çµ±è¨ˆ: {len(successful_results)}ã‚µãƒ³ãƒ—ãƒ«, {feature_vectors.shape[1]}æ¬¡å…ƒ")
    for group_name, group_files in groups.items():
        group_count = len([f for f in group_files if f['file_path'] in file_paths])
        print(f"  {group_name}: {group_count}ãƒ•ã‚¡ã‚¤ãƒ«")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ¯ çµ±åˆç‰¹å¾´é‡æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆCFG + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ï¼‰")

    target_directory = "submissions_typical90_d_15_AC_TLE"
    cache_file = f"feature_cache_{os.path.basename(target_directory)}.json"

    if not os.path.exists(target_directory):
        print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_directory}")
        return

    target_files = find_files_in_directory(target_directory)
    if not target_files:
        print("âš ï¸  å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print(f"ğŸ“ ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}å€‹")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ
    groups = analyze_file_groups(target_files, target_directory)
    print(f"ğŸ“‚ ã‚°ãƒ«ãƒ¼ãƒ—: {', '.join([f'{k}({len(v)})' for k, v in groups.items()])}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†
    batch_results = None
    if os.path.exists(cache_file):
        file_changes = detect_file_changes(target_directory, cache_file)

        if all(len(file_changes[key]) == 0 for key in ['new_files', 'modified_files', 'deleted_files']):
            print(f"ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨: {cache_file}")
            cached_data = load_feature_vectors(cache_file)
            if cached_data:
                batch_results = cached_data['data']
        elif len(file_changes['unchanged_files']) > 0:
            print("ğŸ”„ å¢—åˆ†æ›´æ–°å®Ÿè¡Œ")
            batch_results = update_cache_incrementally(target_directory, cache_file, file_changes)
            save_feature_vectors(batch_results, groups, target_directory, cache_file, format='json')
        else:
            print("ğŸ†• å®Œå…¨å†å®Ÿè¡Œ")

    if batch_results is None:
        print("ğŸ”„ æ–°è¦ç‰¹å¾´é‡æŠ½å‡º")
        batch_results = batch_extract_integrated_features(target_files)
        save_feature_vectors(batch_results, groups, target_directory, cache_file, format='json')

    # çµæœè¡¨ç¤º
    successful = len([r for r in batch_results if 'error' not in r])
    print(f"ğŸ“Š çµæœ: {successful}/{len(batch_results)} æˆåŠŸ")

    # å¯è¦–åŒ–
    try:
        visualize_feature_distribution(batch_results, groups, target_directory)
    except Exception as e:
        print(f"âŒ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

# ä½¿ç”¨ä¾‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã + ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ï¼‰:
#
# from ext_cfg_dfg_feature import (
#     extract_cfg_features_vector,
#     extract_dataflow_features_vector,
#     extract_integrated_features_vector,
#     batch_extract_integrated_features,
#     save_feature_vectors,
#     load_feature_vectors,
#     calculate_pattern_centroids,
#     save_pattern_centroids,
#     load_pattern_centroids
# )
#
# # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
# cfg_vector = extract_cfg_features_vector("submission_1.py")
# print(cfg_vector)  # [connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity]
#
# dataflow_vector = extract_dataflow_features_vector("submission_1.py")
# print(dataflow_vector)  # [total_reads, total_writes, max_reads, max_writes, var_count]
#
# integrated_vector = extract_integrated_features_vector("submission_1.py")
# print(integrated_vector)  # 11æ¬¡å…ƒã®çµ±åˆãƒ™ã‚¯ãƒˆãƒ« [CFG(6æ¬¡å…ƒ) + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼(5æ¬¡å…ƒ)]
#
# # è¤‡æ•°submissionãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†
# submission_files = [f"submission_{i}.py" for i in range(1, 6)]  # submission_1.py to submission_5.py
# batch_results = batch_extract_integrated_features(submission_files)
#
# # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä»˜ãï¼‰
# save_feature_vectors(batch_results, groups, base_directory, "my_features.json", format='json')
# # ã¾ãŸã¯
# save_feature_vectors(batch_results, groups, base_directory, "my_features.pkl", format='pickle')
#
# # ä¿å­˜ã—ãŸçµæœã‚’èª­ã¿è¾¼ã¿
# cached_data = load_feature_vectors("my_features.json")
# if cached_data:
#     cached_results = cached_data['data']
#
#     # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
#     for result in cached_results:
#         if 'error' not in result:
#             print(f"{result['source_file']}: {result['integrated_vector']}")
#         else:
#             print(f"{result['source_file']}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
#
#     # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆk-meansæ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼‰
#     if cached_data.get('pattern_centroids'):
#         centroids_info = cached_data['pattern_centroids']
#         pattern_centroids = []
#         pattern_labels = []
#
#         for pattern_name, centroid_info in centroids_info['centroids'].items():
#             pattern_centroids.append(centroid_info['centroid_vector'])
#             pattern_labels.append(pattern_name)
#
#         # k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ä½¿ç”¨
#         from sklearn.cluster import KMeans
#         kmeans = KMeans(n_clusters=len(pattern_centroids),
#                        init=np.array(pattern_centroids),
#                        n_init=1)  # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãªã®ã§1å›ã§ååˆ†
#         # ã“ã‚Œã§çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’åˆæœŸå€¤ã¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå¯èƒ½
#
# # çµåˆãƒ™ã‚¯ãƒˆãƒ«ã®ã¿ã‚’å–å¾—
# vectors = [r['integrated_vector'] for r in batch_results if 'error' not in r]
# print(f"å–å¾—ã•ã‚ŒãŸçµåˆãƒ™ã‚¯ãƒˆãƒ«æ•°: {len(vectors)}")
# for i, vector in enumerate(vectors):
#     print(f"  submission_{i+1}: {vector}")  # 11æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« [CFG(6æ¬¡å…ƒ) + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼(5æ¬¡å…ƒ)]
