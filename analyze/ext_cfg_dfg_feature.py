# CFGç‰¹å¾´é‡æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ext-cfg-feature.pyã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç°¡æ½”ã«æŠ½å‡º
# ext_feature_data_flow.pyã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã‚‚æŠ½å‡º

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import matplotlib.colors as mcolors
import json
import pickle
from datetime import datetime

# ext-cfg-feature.pyã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    # control-flowãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
    control_flow_path = os.path.join(os.path.dirname(__file__), 'control-flow')
    sys.path.append(control_flow_path)

    from ext_cfg_feature import analyze_accurate_cfg
except ImportError as e:
    print(f"âŒ CFGç‰¹å¾´é‡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ext-cfg-feature.pyãŒ control-flow/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

# ext_feature_data_flow.pyã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    # data-flowãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
    data_flow_path = os.path.join(os.path.dirname(__file__), 'data-flow')
    sys.path.append(data_flow_path)

    from ext_feature_data_flow import extract_dataflow_features_as_list, get_dataflow_feature_vector
except ImportError as e:
    print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ext_feature_data_flow.pyãŒ data-flow/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

def extract_dataflow_features_vector(source_file):
    """
    ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º

    Args:
        source_file (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        list: [total_reads, total_writes, max_reads, max_writes, var_count]
    """
    try:
        # ext_feature_data_flow.pyã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—
        dataflow_vector = get_dataflow_feature_vector(source_file)
        return dataflow_vector

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return [0, 0, 0, 0, 0]

def get_dataflow_feature_names():
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã®åå‰ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    return [
        'total_reads',        # ç·èª­ã¿è¾¼ã¿æ•°
        'total_writes',       # ç·æ›¸ãè¾¼ã¿æ•°
        'max_reads',          # èª­ã¿è¾¼ã¿æ•°æœ€å¤§å€¤
        'max_writes',         # æ›¸ãè¾¼ã¿æ•°æœ€å¤§å€¤
        'var_count'           # å¤‰æ•°ç¨®é¡æ•°
    ]

def extract_integrated_features_vector(source_file):
    """
    CFGç‰¹å¾´é‡ã¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã‚’çµ±åˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º

    Args:
        source_file (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        list: çµ±åˆã•ã‚ŒãŸç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ« [CFG(6æ¬¡å…ƒ) + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼(5æ¬¡å…ƒ)]
    """
    try:
        # CFGç‰¹å¾´é‡ã‚’å–å¾—
        cfg_vector = extract_cfg_features_vector(source_file)

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡ã‚’å–å¾—
        dataflow_vector = extract_dataflow_features_vector(source_file)

        # çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
        integrated_vector = cfg_vector + dataflow_vector

        return integrated_vector

    except Exception as e:
        print(f"âŒ çµ±åˆç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

def extract_cfg_features_vector(source_file):
    """
    ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰CFGç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º

    Args:
        source_file (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        list: [connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity]
    """
    try:
        # print(f"ğŸ”„ CFGç‰¹å¾´é‡æŠ½å‡ºä¸­: {source_file}")

        # ext-cfg-feature.pyã®analyze_accurate_cfgé–¢æ•°ã‚’å‘¼ã³å‡ºã—
        all_features = analyze_accurate_cfg(source_file)

        if not all_features:
            print("âš ï¸  CFGç‰¹å¾´é‡ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return [0, 0, 0, 0, 0, 0]

        # é–¢æ•°ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã¯é™¤å¤–ï¼‰
        function_features = {k: v for k, v in all_features.items()
                           if not (k.startswith('<module>') or k.startswith('&lt;module&gt;'))}

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆæ§‹é€ çš„ç‰¹å¾´ã®ã¿ï¼‰
        module_features = {k: v for k, v in all_features.items()
                         if k.startswith('<module>') or k.startswith('&lt;module&gt;')}

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        # 1. connected_components: è«–ç†ç©ï¼ˆ1ã¤ã§ã‚‚0ãŒã‚ã‚Œã°0ã€å…¨ã¦1ä»¥ä¸Šãªã‚‰1ï¼‰
        all_connected_components = [features.get('connected_components', 0) for features in all_features.values()]
        total_connected = 1 if all(cc > 0 for cc in all_connected_components) else 0

        # 2. ãƒ«ãƒ¼ãƒ—ã¨æ¡ä»¶æ–‡: é–¢æ•°å˜ä½ã§æ—¢ã«æ­£ç¢ºã«è¨ˆç®—æ¸ˆã¿ï¼ˆå†å¸°å«ã‚€ï¼‰
        total_loops = sum(features.get('loop_statements', 0) for features in function_features.values())
        total_conditions = sum(features.get('conditional_statements', 0) for features in function_features.values())

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®åˆ†ã‚‚è¿½åŠ 
        total_loops += sum(features.get('loop_statements', 0) for features in module_features.values())
        total_conditions += sum(features.get('conditional_statements', 0) for features in module_features.values())

        # 3. æ§‹é€ çš„ç‰¹å¾´: å…¨ä½“ã‹ã‚‰è¨ˆç®—ï¼ˆé–¢æ•°+ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
        total_cycles = sum(features.get('cycles', 0) for features in all_features.values())
        total_paths = sum(features.get('paths', 0) for features in all_features.values())
        total_complexity = sum(features.get('cyclomatic_complexity', 0) for features in all_features.values())

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
        clustering_vector = [
            total_connected,     # 1. é€£çµæˆåˆ†æ•°
            total_loops,         # 2. ãƒ«ãƒ¼ãƒ—æ–‡æ•°ï¼ˆå†å¸°å«ã‚€ï¼‰
            total_conditions,    # 3. æ¡ä»¶æ–‡æ•°
            total_cycles,        # 4. ã‚µã‚¤ã‚¯ãƒ«æ•°
            total_paths,         # 5. ãƒ‘ã‚¹æ•°ï¼ˆãƒ«ãƒ¼ãƒ—è€ƒæ…®ç‰ˆï¼‰
            total_complexity     # 6. ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦
        ]

        # print(f"âœ… CFGç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {clustering_vector}")
        return clustering_vector

    except Exception as e:
        print(f"âŒ CFGç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return [0, 0, 0, 0, 0, 0]

def get_cfg_feature_names():
    """CFGç‰¹å¾´é‡ã®åå‰ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    return [
        'connected_components',    # é€£çµæˆåˆ†æ•°
        'loop_statements',         # ãƒ«ãƒ¼ãƒ—æ–‡æ•°ï¼ˆå†å¸°å«ã‚€ï¼‰
        'conditional_statements',  # æ¡ä»¶æ–‡æ•°
        'cycles',                  # ã‚µã‚¤ã‚¯ãƒ«æ•°
        'paths',                   # ãƒ‘ã‚¹æ•°ï¼ˆãƒ«ãƒ¼ãƒ—è€ƒæ…®ç‰ˆï¼‰
        'cyclomatic_complexity'    # ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦
    ]

def batch_extract_integrated_features(file_list):
    """
    è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆç‰¹å¾´é‡ã‚’ä¸€æ‹¬æŠ½å‡º

    Args:
        file_list (list): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

    Returns:
        list: å„ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ãƒªã‚¹ãƒˆ
    """
    results = []

    print(f"ğŸ“‚ çµ±åˆç‰¹å¾´é‡ä¸€æ‹¬æŠ½å‡ºé–‹å§‹ ({len(file_list)}ãƒ•ã‚¡ã‚¤ãƒ«)")

    for i, source_file in enumerate(file_list, 1):
        try:
            result = extract_integrated_features_vector(source_file)
            results.append({
                'source_file': source_file,
                'integrated_vector': result
            })

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
            results.append({
                'source_file': source_file,
                'integrated_vector': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                'error': str(e)
            })

    return results

def batch_extract_cfg_features(file_list):
    """
    è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®CFGç‰¹å¾´é‡ã‚’ä¸€æ‹¬æŠ½å‡º

    Args:
        file_list (list): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

    Returns:
        list: å„ãƒ•ã‚¡ã‚¤ãƒ«ã®CFGç‰¹å¾´é‡çµæœãƒªã‚¹ãƒˆ
    """
    results = []

    print(f"ğŸ“‚ CFGç‰¹å¾´é‡ä¸€æ‹¬æŠ½å‡ºé–‹å§‹ ({len(file_list)}ãƒ•ã‚¡ã‚¤ãƒ«)")

    for i, source_file in enumerate(file_list, 1):
        try:
            result = extract_cfg_features_vector(source_file)
            results.append(result)

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
            error_result = {
                'source_file': source_file,
                'clustering_vector': [0, 0, 0, 0, 0, 0],
                'feature_names': get_cfg_feature_names(),
                'individual_features': {},
                'summary': {'total_functions': 0, 'total_modules': 0},
                'error': str(e)
            }
            results.append(error_result)

    return results

def find_files_in_directory(directory, file_extensions=['.py', '.c', '.cpp', '.java']):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹

    Args:
        directory (str): æ¤œç´¢å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        file_extensions (list): å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒªã‚¹ãƒˆ

    Returns:
        list: ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    found_files = []

    def explore_directory(current_dir):
        try:
            # lsã‚³ãƒãƒ³ãƒ‰ç›¸å½“: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’å–å¾—
            contents = os.listdir(current_dir)

            # .ã‹ã‚‰å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é™¤å¤–
            filtered_contents = [item for item in contents if not item.startswith('.')]

            for item in filtered_contents:
                item_path = os.path.join(current_dir, item)

                if os.path.isfile(item_path):
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ: æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
                    _, ext = os.path.splitext(item)
                    if ext.lower() in file_extensions:
                        found_files.append(item_path)

                elif os.path.isdir(item_path):
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ: å†å¸°çš„ã«æ¢ç´¢
                    explore_directory(item_path)

        except (FileNotFoundError, PermissionError) as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {current_dir} - {e}")

    explore_directory(directory)

    return found_files

def analyze_file_groups(file_list, base_directory):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’patternãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã—ã¦åˆ†æ

    Args:
        file_list (list): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

    Returns:
        dict: ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœ
    """
    groups = {}

    for file_path in file_list:
        relative_path = os.path.relpath(file_path, base_directory)
        path_parts = relative_path.split(os.sep)

        # patternãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã©ã†ã‹ã‚’åˆ¤å®š
        if len(path_parts) > 1 and path_parts[0].startswith('pattern'):
            group_name = path_parts[0]  # pattern_1, pattern_2, etc.
        else:
            group_name = 'other'  # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«

        if group_name not in groups:
            groups[group_name] = []

        groups[group_name].append({
            'file_path': file_path,
            'relative_path': relative_path,
            'filename': os.path.basename(file_path)
        })

    return groups

def save_feature_vectors(batch_results, groups=None, base_directory=None, output_file=None, format='json'):
    """
    ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚‚å«ã‚€ï¼‰

    Args:
        batch_results (list): ç‰¹å¾´é‡æŠ½å‡ºçµæœ
        groups (dict): ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ç”¨ï¼‰
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ç”¨ï¼‰
        output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        format (str): ä¿å­˜å½¢å¼ ('json' ã¾ãŸã¯ 'pickle')
    """
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if format == 'json':
            output_file = f"feature_vectors_{timestamp}.json"
        else:
            output_file = f"feature_vectors_{timestamp}.pkl"

    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        save_data = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(batch_results),
            'successful_extractions': len([r for r in batch_results if 'error' not in r]),
            'feature_names': {
                'cfg_features': get_cfg_feature_names(),
                'dataflow_features': get_dataflow_feature_names()
            },
            'data': batch_results
        }

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚‚è¨ˆç®—ãƒ»ä¿å­˜
        if groups is not None and base_directory is not None:
            print("ğŸ¯ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’è¨ˆç®—ã—ã¦è¿½åŠ ä¸­...")
            centroids_data = calculate_pattern_centroids(batch_results, groups, base_directory)

            if centroids_data and centroids_data['centroids']:
                save_data['pattern_centroids'] = centroids_data
                print(f"âœ… {len(centroids_data['centroids'])}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

                # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ¦‚è¦ã‚’è¡¨ç¤º
                for pattern_name, centroid_info in centroids_data['centroids'].items():
                    centroid = centroid_info['centroid_vector']
                    count = centroid_info['sample_count']
                    print(f"   {pattern_name}: {count}ãƒ•ã‚¡ã‚¤ãƒ« â†’ é‡å¿ƒ[{', '.join([f'{x:.3f}' for x in centroid[:3]])}...]")
            else:
                save_data['pattern_centroids'] = None
                print("âš ï¸ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒè¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        else:
            save_data['pattern_centroids'] = None

        if format == 'json':
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
        elif format == 'pickle':
            with open(output_file, 'wb') as f:
                pickle.dump(save_data, f)
        else:
            raise ValueError("format ã¯ 'json' ã¾ãŸã¯ 'pickle' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

        print(f"ğŸ’¾ ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«{'ã¨ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰' if save_data['pattern_centroids'] else ''}ã‚’ '{output_file}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        print(f"   å½¢å¼: {format.upper()}")
        print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {save_data['total_files']}")
        print(f"   æˆåŠŸæ•°: {save_data['successful_extractions']}")
        if save_data['pattern_centroids']:
            print(f"   ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ•°: {len(save_data['pattern_centroids']['centroids'])}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³")

        return output_file

    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_feature_vectors(input_file):
    """
    ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿

    Args:
        input_file (str): å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        dict: èª­ã¿è¾¼ã¾ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    """
    try:
        if input_file.endswith('.json'):
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        elif input_file.endswith('.pkl'):
            with open(input_file, 'rb') as f:
                data = pickle.load(f)
        else:
            raise ValueError("ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆ.json ã¾ãŸã¯ .pkl ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰")

        print(f"ğŸ“‚ ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ '{input_file}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        print(f"   ä¿å­˜æ—¥æ™‚: {data['timestamp']}")
        print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {data['total_files']}")
        print(f"   æˆåŠŸæ•°: {data['successful_extractions']}")

        return data

    except Exception as e:
        print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def calculate_pattern_centroids(batch_results, groups, base_directory):
    """
    å„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é‡å¿ƒï¼ˆçœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼‰ã‚’è¨ˆç®—

    Args:
        batch_results (list): ç‰¹å¾´é‡æŠ½å‡ºçµæœ
        groups (dict): ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœ
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

    Returns:
        dict: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±
    """
    print("ğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é‡å¿ƒï¼ˆçœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼‰è¨ˆç®—ä¸­...")

    # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
    successful_results = [r for r in batch_results if 'error' not in r]

    if len(successful_results) == 0:
        print("âŒ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return {}

    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    feature_vectors = np.array([r['integrated_vector'] for r in successful_results])
    file_paths = [r['source_file'] for r in successful_results]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    file_to_group = {}
    for group_name, group_files in groups.items():
        for file_info in group_files:
            file_to_group[file_info['file_path']] = group_name

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹
    pattern_groups = {k: v for k, v in groups.items() if k.startswith('pattern')}

    # "other"ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆrootãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚‚å«ã‚ã‚‹
    if 'other' in groups and len(groups['other']) > 0:
        pattern_groups['other'] = groups['other']
        print(f"ğŸ“ 'other'ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆrootãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚‚çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã«å«ã‚ã¾ã™: {len(groups['other'])}ãƒ•ã‚¡ã‚¤ãƒ«")

    centroids_data = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'base_directory': base_directory,
            'total_patterns': len(pattern_groups),
            'includes_other_group': 'other' in pattern_groups,
            'feature_dimension': len(feature_vectors[0]) if len(feature_vectors) > 0 else 0,
            'feature_names': {
                'cfg_features': get_cfg_feature_names(),
                'dataflow_features': get_dataflow_feature_names()
            }
        },
        'centroids': {}
    }

    print(f"ğŸ“Š {len(pattern_groups)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’è¨ˆç®—ä¸­...")

    for pattern_name, pattern_files in pattern_groups.items():
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å±ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        pattern_indices = []
        pattern_file_paths = []

        for i, file_path in enumerate(file_paths):
            if file_to_group.get(file_path) == pattern_name:
                pattern_indices.append(i)
                pattern_file_paths.append(file_path)

        if not pattern_indices:
            print(f"âš ï¸ {pattern_name}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
        pattern_vectors = feature_vectors[pattern_indices]

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼ˆé‡å¿ƒï¼‰ã‚’è¨ˆç®—
        centroid = np.mean(pattern_vectors, axis=0).tolist()

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’ä¿å­˜ï¼ˆçµ±è¨ˆæƒ…å ±ã¯å‰Šé™¤ï¼‰
        centroids_data['centroids'][pattern_name] = {
            'centroid_vector': centroid,
            'sample_count': len(pattern_indices),
            'file_paths': pattern_file_paths
        }

        print(f"âœ… {pattern_name}: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—å®Œäº† ({len(pattern_indices)} ãƒ•ã‚¡ã‚¤ãƒ«)")
        print(f"   é‡å¿ƒãƒ™ã‚¯ãƒˆãƒ«: {[f'{x:.3f}' for x in centroid[:6]][:3]}...")  # æœ€åˆã®3è¦ç´ ã®ã¿è¡¨ç¤º

    return centroids_data

def save_pattern_centroids(centroids_data, output_file=None):
    """
    ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        centroids_data (dict): ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿
        output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰

    Returns:
        str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
    """
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_dir = centroids_data['metadata'].get('base_directory', 'unknown')
        base_name = os.path.basename(base_dir) if base_dir != 'unknown' else 'patterns'
        output_file = f"pattern_centroids_{base_name}_{timestamp}.json"

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(centroids_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ '{output_file}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {centroids_data['metadata']['total_patterns']}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {centroids_data['metadata']['feature_dimension']}")

        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä¸€è¦§ã‚’è¡¨ç¤º
        print(f"ğŸ“‹ ä¿å­˜ã•ã‚ŒãŸã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰:")
        for pattern_name, centroid_info in centroids_data['centroids'].items():
            sample_count = centroid_info['sample_count']
            centroid_vector = centroid_info['centroid_vector']
            print(f"   {pattern_name}: {sample_count}ãƒ•ã‚¡ã‚¤ãƒ« â†’ [{', '.join([f'{x:.3f}' for x in centroid_vector[:3]])}...]")

        return output_file

    except Exception as e:
        print(f"âŒ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_pattern_centroids(input_file):
    """
    ä¿å­˜ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿

    Args:
        input_file (str): å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        dict: èª­ã¿è¾¼ã¾ã‚ŒãŸã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿
    """
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"ğŸ“‚ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ '{input_file}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        print(f"   ä¿å­˜æ—¥æ™‚: {data['metadata']['timestamp']}")
        print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {data['metadata']['total_patterns']}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {data['metadata']['feature_dimension']}")

        return data

    except Exception as e:
        print(f"âŒ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def check_cache_validity(target_directory, cache_file):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        target_directory (str): å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        cache_file (str): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        bool: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹
    """
    if not os.path.exists(cache_file):
        return False

    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’å–å¾—
        cache_mtime = os.path.getmtime(cache_file)

        # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€æ–°æ›´æ–°æ™‚åˆ»ã‚’ãƒã‚§ãƒƒã‚¯
        latest_file_mtime = 0
        for root, dirs, files in os.walk(target_directory):
            for file in files:
                if file.endswith(('.py', '.c', '.cpp', '.java')):
                    file_path = os.path.join(root, file)
                    file_mtime = os.path.getmtime(file_path)
                    latest_file_mtime = max(latest_file_mtime, file_mtime)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚ˆã‚Šæ–°ã—ã‘ã‚Œã°æœ‰åŠ¹
        return cache_mtime > latest_file_mtime

    except Exception as e:
        print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def visualize_feature_distribution(batch_results, groups, base_directory):
    """
    ç‰¹å¾´é‡ã®åˆ†å¸ƒã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«å¯è¦–åŒ–ï¼ˆå…¨ä½“ï¼‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å€‹åˆ¥ãƒ—ãƒ­ãƒƒãƒˆï¼‰

    Args:
        batch_results (list): ç‰¹å¾´é‡æŠ½å‡ºçµæœ
        groups (dict): ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æçµæœ
        base_directory (str): ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
    """
    # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’ä½¿ç”¨
    successful_results = [r for r in batch_results if 'error' not in r]

    if len(successful_results) == 0:
        print("âŒ å¯è¦–åŒ–å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    feature_vectors = np.array([r['integrated_vector'] for r in successful_results])
    file_paths = [r['source_file'] for r in successful_results]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    file_to_group = {}
    group_colors = {}
    color_palette = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'olive', 'cyan']

    # otherã‚°ãƒ«ãƒ¼ãƒ—ã¯ç°è‰²ã«è¨­å®š
    color_idx = 0
    for group_name, group_files in groups.items():
        if group_name == 'other':
            group_colors[group_name] = 'gray'
        else:
            group_colors[group_name] = color_palette[color_idx % len(color_palette)]
            color_idx += 1

        for file_info in group_files:
            file_to_group[file_info['file_path']] = group_name

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è‰²ã¨ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
    colors = []
    labels = []
    for file_path in file_paths:
        group = file_to_group.get(file_path, 'other')
        colors.append(group_colors[group])
        labels.append(group)

    # PCAã§2æ¬¡å…ƒã«æ¬¡å…ƒå‰Šæ¸›
    print("ğŸ“Š PCAã§æ¬¡å…ƒå‰Šæ¸›ä¸­...")
    pca = PCA(n_components=2)
    feature_vectors_2d = pca.fit_transform(feature_vectors)

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆï¼ˆå…¨ãƒ—ãƒ­ãƒƒãƒˆã§å…±é€šï¼‰
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_dir = f"feature_visualization_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ å¯è¦–åŒ–çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

    # 1. å…¨ä½“ãƒ—ãƒ­ãƒƒãƒˆï¼ˆã™ã¹ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å«ã‚€ï¼‰
    print("ğŸ¨ å…¨ä½“ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
    plt.figure(figsize=(12, 8))

    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ãƒ—ãƒ­ãƒƒãƒˆ
    for group_name in groups.keys():
        group_indices = [i for i, label in enumerate(labels) if label == group_name]
        if group_indices:
            group_points = feature_vectors_2d[group_indices]
            plt.scatter(group_points[:, 0], group_points[:, 1],
                       c=group_colors[group_name],
                       label=f'{group_name} ({len(group_indices)} files)',
                       alpha=0.7, s=60)

    plt.title(f'Feature Distribution Visualization (All Patterns)\n{base_directory}', fontsize=14)
    plt.xlabel(f'PC1 (Explained Variance: {pca.explained_variance_ratio_[0]:.2%})', fontsize=12)
    plt.ylabel(f'PC2 (Explained Variance: {pca.explained_variance_ratio_[1]:.2%})', fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    total_variance = sum(pca.explained_variance_ratio_)
    plt.figtext(0.02, 0.02, f'Total Explained Variance: {total_variance:.2%}', fontsize=10)

    plt.tight_layout()

    # å…¨ä½“ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜
    all_filename = os.path.join(output_dir, f"feature_distribution_all_{timestamp}.png")
    plt.savefig(all_filename, dpi=150, bbox_inches='tight')
    print(f"ğŸ“¸ å…¨ä½“ãƒ—ãƒ­ãƒƒãƒˆã‚’ '{all_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

    plt.show()

    # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å€‹åˆ¥ãƒ—ãƒ­ãƒƒãƒˆ
    pattern_groups = {k: v for k, v in groups.items() if k.startswith('pattern')}

    if pattern_groups:
        print(f"\nğŸ¨ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å€‹åˆ¥ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­... ({len(pattern_groups)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³)")

        for pattern_name, pattern_files in pattern_groups.items():
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å±ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            pattern_indices = [i for i, label in enumerate(labels) if label == pattern_name]

            if not pattern_indices:
                print(f"âš ï¸ {pattern_name}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            pattern_vectors_2d = feature_vectors_2d[pattern_indices]
            pattern_vectors_original = feature_vectors[pattern_indices]

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥PCAã‚’å®Ÿè¡Œï¼ˆãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ï¼‰
            if len(pattern_indices) > 1:  # 2ã¤ä»¥ä¸Šã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¿…è¦
                pattern_pca = PCA(n_components=min(2, len(pattern_indices)-1))
                try:
                    pattern_vectors_pca = pattern_pca.fit_transform(pattern_vectors_original)
                    use_pattern_pca = True
                    pattern_explained_var = pattern_pca.explained_variance_ratio_
                except:
                    # PCAãŒå¤±æ•—ã—ãŸå ´åˆã¯å…¨ä½“PCAã®çµæœã‚’ä½¿ç”¨
                    pattern_vectors_pca = pattern_vectors_2d
                    use_pattern_pca = False
                    pattern_explained_var = [0, 0]
            else:
                pattern_vectors_pca = pattern_vectors_2d
                use_pattern_pca = False
                pattern_explained_var = [0, 0]

            # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
            plt.figure(figsize=(10, 8))

            # ãƒ‘ã‚¿ãƒ¼ãƒ³å†…ã§ã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚ˆã‚‹è‰²åˆ†ã‘ï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            pattern_color_base = group_colors[pattern_name]
            n_files = len(pattern_indices)

            if n_files > 1:
                # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                cmap = plt.cm.get_cmap('viridis')
                colors_pattern = [cmap(i / (n_files - 1)) for i in range(n_files)]
            else:
                colors_pattern = [pattern_color_base]

            scatter = plt.scatter(pattern_vectors_pca[:, 0], pattern_vectors_pca[:, 1],
                                c=colors_pattern, s=100, alpha=0.8, edgecolors='black', linewidth=0.5)

            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            pattern_file_paths = [file_paths[i] for i in pattern_indices]
            for i, (x, y) in enumerate(pattern_vectors_pca):
                filename = os.path.basename(pattern_file_paths[i])
                plt.annotate(filename, (x, y), xytext=(5, 5), textcoords='offset points',
                           fontsize=8, alpha=0.8)

            # ã‚¿ã‚¤ãƒˆãƒ«ã¨è»¸ãƒ©ãƒ™ãƒ«
            pca_info = ""
            if use_pattern_pca and len(pattern_explained_var) >= 2:
                pca_info = f"\nPattern-specific PCA: PC1={pattern_explained_var[0]:.2%}, PC2={pattern_explained_var[1]:.2%}"
            else:
                pca_info = f"\nUsing global PCA projection"

            plt.title(f'{pattern_name.upper()} Feature Distribution\n{len(pattern_files)} files{pca_info}',
                     fontsize=14)

            if use_pattern_pca:
                plt.xlabel(f'{pattern_name} PC1', fontsize=12)
                plt.ylabel(f'{pattern_name} PC2', fontsize=12)
            else:
                plt.xlabel(f'Global PC1', fontsize=12)
                plt.ylabel(f'Global PC2', fontsize=12)

            plt.grid(True, alpha=0.3)

            # çµ±è¨ˆæƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹
            stats_text = f"Files: {n_files}\n"
            if len(pattern_vectors_original) > 0:
                avg_vector = np.mean(pattern_vectors_original, axis=0)
                std_vector = np.std(pattern_vectors_original, axis=0)
                stats_text += f"Avg complexity: {avg_vector[5]:.1f}\n"  # cyclomatic_complexity
                stats_text += f"Avg paths: {avg_vector[4]:.1f}"  # paths

            plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,
                    fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            plt.tight_layout()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜
            pattern_filename = os.path.join(output_dir, f"feature_distribution_{pattern_name}_{timestamp}.png")
            plt.savefig(pattern_filename, dpi=150, bbox_inches='tight')
            print(f"ğŸ“¸ {pattern_name}ãƒ—ãƒ­ãƒƒãƒˆã‚’ '{pattern_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

            plt.show()

    # 3. æ¯”è¼ƒã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼‰
    if len(pattern_groups) > 1:
        print(f"\nğŸ¨ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
        plt.figure(figsize=(12, 8))

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼ˆå¹³å‡ï¼‰ã‚’è¨ˆç®—
        pattern_centroids = []
        pattern_names_list = []
        pattern_colors_list = []

        for pattern_name, pattern_files in pattern_groups.items():
            pattern_indices = [i for i, label in enumerate(labels) if label == pattern_name]
            if pattern_indices:
                pattern_vectors = feature_vectors[pattern_indices]
                centroid = np.mean(pattern_vectors, axis=0)
                pattern_centroids.append(centroid)
                pattern_names_list.append(pattern_name)
                pattern_colors_list.append(group_colors[pattern_name])

        if pattern_centroids:
            # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’PCAã§å¯è¦–åŒ–
            centroids_array = np.array(pattern_centroids)
            centroids_2d = pca.transform(centroids_array)

            # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            scatter = plt.scatter(centroids_2d[:, 0], centroids_2d[:, 1],
                                c=pattern_colors_list, s=200, alpha=0.8,
                                edgecolors='black', linewidth=2, marker='D')

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            for i, (x, y) in enumerate(centroids_2d):
                plt.annotate(pattern_names_list[i], (x, y), xytext=(10, 10),
                           textcoords='offset points', fontsize=12, fontweight='bold')

            plt.title(f'Pattern Centroids Comparison\n{base_directory}', fontsize=14)
            plt.xlabel(f'PC1 (Explained Variance: {pca.explained_variance_ratio_[0]:.2%})', fontsize=12)
            plt.ylabel(f'PC2 (Explained Variance: {pca.explained_variance_ratio_[1]:.2%})', fontsize=12)
            plt.grid(True, alpha=0.3)

            plt.tight_layout()

            # æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜
            comparison_filename = os.path.join(output_dir, f"feature_comparison_centroids_{timestamp}.png")
            plt.savefig(comparison_filename, dpi=150, bbox_inches='tight')
            print(f"ğŸ“¸ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ '{comparison_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

            plt.show()

    # è©³ç´°çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    print(f"\nğŸ“ˆ ç‰¹å¾´é‡åˆ†å¸ƒçµ±è¨ˆ:")
    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(successful_results)}")
    print(f"  ç‰¹å¾´é‡æ¬¡å…ƒæ•°: {feature_vectors.shape[1]}")
    print(f"  PCAèª¬æ˜åˆ†æ•£: PC1={pca.explained_variance_ratio_[0]:.2%}, PC2={pca.explained_variance_ratio_[1]:.2%}")

    for group_name, group_files in groups.items():
        group_count = len([f for f in group_files if f['file_path'] in file_paths])
        print(f"  {group_name}: {group_count} ãƒ•ã‚¡ã‚¤ãƒ« (è‰²: {group_colors[group_name]})")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ‹¬å‡¦ç†ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
    print("ğŸ¯ çµ±åˆç‰¹å¾´é‡æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆCFG + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ï¼‰")

    # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
    # target_directory = r"C:\Users\yxicu\python\pyjoern\atcoder\submissions_typical90_d_100"
    target_directory = "../atcoder/submissions_typical90_d_100"

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    cache_file = f"feature_cache_{os.path.basename(target_directory)}.json"

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(target_directory):
        print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_directory}")
        print("å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    # lsã‚³ãƒãƒ³ãƒ‰é¢¨ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç™ºè¦‹
    target_files = find_files_in_directory(target_directory)

    if not target_files:
        print("âš ï¸  å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    print(f"\nğŸ“ ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}å€‹")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†å‰ã«å®Ÿè¡Œï¼‰
    print(f"\nğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æä¸­...")
    groups = analyze_file_groups(target_files, target_directory)

    print(f"ğŸ“‚ ç™ºè¦‹ã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—:")
    for group_name, group_files in groups.items():
        print(f"  {group_name}: {len(group_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        for file_info in group_files[:3]:  # æœ€åˆã®3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
            print(f"    - {file_info['relative_path']}")
        if len(group_files) > 3:
            print(f"    ... ãŠã‚ˆã³ {len(group_files) - 3} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    use_cache = False
    if os.path.exists(cache_file):
        if check_cache_validity(target_directory, cache_file):
            print(f"ğŸ“¦ æœ‰åŠ¹ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {cache_file}")
            use_cache_input = input("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
            use_cache = use_cache_input in ['y', 'yes', '']
        else:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¤ã„ãŸã‚ã€å†æŠ½å‡ºãŒå¿…è¦ã§ã™")

    batch_results = None
    cached_data = None

    if use_cache:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        print(f"ğŸ“‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        cached_data = load_feature_vectors(cache_file)
        if cached_data:
            batch_results = cached_data['data']
            print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ {len(batch_results)} ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    if batch_results is None:
        # æ–°è¦æŠ½å‡º
        for i, file in enumerate(target_files, 1):
            relative_path = os.path.relpath(file, target_directory)
            print(f"  {i:2d}. {relative_path}")

        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†ã‚’å®Ÿè¡Œ
        print(f"\nğŸ”„ è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ‹¬å‡¦ç†é–‹å§‹")
        batch_results = batch_extract_integrated_features(target_files)

        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        print(f"\nğŸ’¾ ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ä¸­...")
        save_feature_vectors(batch_results, groups, target_directory, cache_file, format='json')

        # æ–°è¦ä½œæˆã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ cached_data ã‚’è¨­å®š
        cached_data = load_feature_vectors(cache_file)

    # çµæœã‚’è¡¨ç¤º
    print(f"\nğŸ“Š ä¸€æ‹¬å‡¦ç†çµæœ:")
    for i, result in enumerate(batch_results, 1):
        filename = os.path.basename(result['source_file'])
        relative_path = os.path.relpath(result['source_file'], target_directory)

        if 'error' in result:
            print(f"  {i:2d}. âŒ {relative_path}: ã‚¨ãƒ©ãƒ¼")
        else:
            print(f"  {i:2d}. âœ… {relative_path}: {result['integrated_vector']}")

    # ç‰¹å¾´é‡åˆ†å¸ƒã®å¯è¦–åŒ–
    print(f"\nğŸ¨ ç‰¹å¾´é‡åˆ†å¸ƒå¯è¦–åŒ–é–‹å§‹...")
    try:
        visualize_feature_distribution(batch_results, groups, target_directory)
    except ImportError as e:
        print(f"âŒ å¯è¦–åŒ–ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("pip install matplotlib scikit-learn numpy")
    except Exception as e:
        print(f"âŒ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼ˆçœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼‰ã‚’è¨ˆç®—ãƒ»ä¿å­˜
    print(f"\nğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ãƒ»ä¿å­˜é–‹å§‹...")
    try:
        # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if batch_results and cached_data and cached_data.get('pattern_centroids'):
            print("âœ… ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ã¾ã™")
            centroids_data = cached_data['pattern_centroids']

            # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ¦‚è¦ã‚’è¡¨ç¤º
            print(f"\nğŸ“ˆ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ¦‚è¦ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰:")
            for pattern_name, centroid_info in centroids_data['centroids'].items():
                centroid = centroid_info['centroid_vector']
                count = centroid_info['sample_count']
                print(f"   {pattern_name} ({count}ãƒ•ã‚¡ã‚¤ãƒ«):")
                print(f"     CFGç‰¹å¾´é‡: [{', '.join([f'{x:.2f}' for x in centroid[:6]])}]")
                print(f"     ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡: [{', '.join([f'{x:.2f}' for x in centroid[6:]])}]")
        else:
            # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’æ–°è¦è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
            print("ğŸ”„ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ ä¸­...")
            centroids_data = calculate_pattern_centroids(batch_results, groups, target_directory)

            if centroids_data and centroids_data['centroids']:
                # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                try:
                    if os.path.exists(cache_file):
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                    else:
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆç”¨ãƒ‡ãƒ¼ã‚¿
                        cache_data = {
                            'timestamp': datetime.now().isoformat(),
                            'total_files': len(batch_results),
                            'successful_extractions': len([r for r in batch_results if 'error' not in r]),
                            'feature_names': {
                                'cfg_features': get_cfg_feature_names(),
                                'dataflow_features': get_dataflow_feature_names()
                            },
                            'data': batch_results
                        }

                    # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
                    cache_data['pattern_centroids'] = centroids_data

                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, indent=2, ensure_ascii=False)

                    print(f"âœ… ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ« '{cache_file}' ã«è¿½åŠ ã—ã¾ã—ãŸ")
                    print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(centroids_data['centroids'])}å€‹")

                    # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ¦‚è¦ã‚’è¡¨ç¤º
                    print(f"\nğŸ“ˆ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æ¦‚è¦:")
                    for pattern_name, centroid_info in centroids_data['centroids'].items():
                        centroid = centroid_info['centroid_vector']
                        count = centroid_info['sample_count']
                        print(f"   {pattern_name} ({count}ãƒ•ã‚¡ã‚¤ãƒ«):")
                        print(f"     CFGç‰¹å¾´é‡: [{', '.join([f'{x:.2f}' for x in centroid[:6]])}]")
                        print(f"     ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç‰¹å¾´é‡: [{', '.join([f'{x:.2f}' for x in centroid[6:]])}]")

                except Exception as file_error:
                    print(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚¨ãƒ©ãƒ¼: {file_error}")
                    print("âš ï¸ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰æƒ…å ±ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™")
                    centroid_file = save_pattern_centroids(centroids_data)
                    if centroid_file:
                        print(f"âœ… ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ç”ŸæˆæˆåŠŸ: {centroid_file}")
            else:
                print("âš ï¸ è¨ˆç®—å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

    # æ‰‹å‹•ä¿å­˜ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    save_option = input("\nğŸ’¾ çµæœã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
    if save_option in ['y', 'yes']:
        format_option = input("ä¿å­˜å½¢å¼ã‚’é¸æŠã—ã¦ãã ã•ã„ (json/pickle): ").lower().strip()
        if format_option in ['json', 'pickle']:
            save_feature_vectors(batch_results, groups, target_directory, format=format_option)
        else:
            print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§JSONã§ä¿å­˜ã—ã¾ã™")
            save_feature_vectors(batch_results, groups, target_directory, format='json')

if __name__ == "__main__":
    main()

# ä½¿ç”¨ä¾‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã + ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰è¨ˆç®—ï¼‰:
#
# from ext_cfg_dfg_feature import (
#     extract_cfg_features_vector,
#     extract_dataflow_features_vector,
#     extract_integrated_features_vector,
#     batch_extract_integrated_features,
#     save_feature_vectors,
#     load_feature_vectors,
#     calculate_pattern_centroids,
#     save_pattern_centroids,
#     load_pattern_centroids
# )
#
# # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
# cfg_vector = extract_cfg_features_vector("submission_1.py")
# print(cfg_vector)  # [connected_components, loop_statements, conditional_statements, cycles, paths, cyclomatic_complexity]
#
# dataflow_vector = extract_dataflow_features_vector("submission_1.py")
# print(dataflow_vector)  # [total_reads, total_writes, max_reads, max_writes, var_count]
#
# integrated_vector = extract_integrated_features_vector("submission_1.py")
# print(integrated_vector)  # 11æ¬¡å…ƒã®çµ±åˆãƒ™ã‚¯ãƒˆãƒ« [CFG(6æ¬¡å…ƒ) + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼(5æ¬¡å…ƒ)]
#
# # è¤‡æ•°submissionãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†
# submission_files = [f"submission_{i}.py" for i in range(1, 6)]  # submission_1.py to submission_5.py
# batch_results = batch_extract_integrated_features(submission_files)
#
# # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä»˜ãï¼‰
# save_feature_vectors(batch_results, groups, base_directory, "my_features.json", format='json')
# # ã¾ãŸã¯
# save_feature_vectors(batch_results, groups, base_directory, "my_features.pkl", format='pickle')
#
# # ä¿å­˜ã—ãŸçµæœã‚’èª­ã¿è¾¼ã¿
# cached_data = load_feature_vectors("my_features.json")
# if cached_data:
#     cached_results = cached_data['data']
#
#     # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
#     for result in cached_results:
#         if 'error' not in result:
#             print(f"{result['source_file']}: {result['integrated_vector']}")
#         else:
#             print(f"{result['source_file']}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
#
#     # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆk-meansæ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼‰
#     if cached_data.get('pattern_centroids'):
#         centroids_info = cached_data['pattern_centroids']
#         pattern_centroids = []
#         pattern_labels = []
#
#         for pattern_name, centroid_info in centroids_info['centroids'].items():
#             pattern_centroids.append(centroid_info['centroid_vector'])
#             pattern_labels.append(pattern_name)
#
#         # k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ä½¿ç”¨
#         from sklearn.cluster import KMeans
#         kmeans = KMeans(n_clusters=len(pattern_centroids),
#                        init=np.array(pattern_centroids),
#                        n_init=1)  # çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ãªã®ã§1å›ã§ååˆ†
#         # ã“ã‚Œã§çœŸã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’åˆæœŸå€¤ã¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå¯èƒ½
#
# # çµåˆãƒ™ã‚¯ãƒˆãƒ«ã®ã¿ã‚’å–å¾—
# vectors = [r['integrated_vector'] for r in batch_results if 'error' not in r]
# print(f"å–å¾—ã•ã‚ŒãŸçµåˆãƒ™ã‚¯ãƒˆãƒ«æ•°: {len(vectors)}")
# for i, vector in enumerate(vectors):
#     print(f"  submission_{i+1}: {vector}")  # 11æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« [CFG(6æ¬¡å…ƒ) + ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼(5æ¬¡å…ƒ)]
